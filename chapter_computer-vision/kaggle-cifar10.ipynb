{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Image Classification (CIFAR-10) on Kaggle\n",
    ":label:`sec_kaggle_cifar10`\n",
    "\n",
    "So far, we have been using Gluon's `data` package to directly obtain image datasets in the tensor format. In practice, however, image datasets often exist in the format of image files. In this section, we will start with the original image files and organize, read, and convert the files to the tensor format step by step.\n",
    "\n",
    "We performed an experiment on the CIFAR-10 dataset in :numref:`sec_image_augmentation`.\n",
    "This is an important data\n",
    "set in the computer vision field. Now, we will apply the knowledge we learned in\n",
    "the previous sections in order to participate in the Kaggle competition, which\n",
    "addresses CIFAR-10 image classification problems. The competition's web address\n",
    "is\n",
    "\n",
    "> https://www.kaggle.com/c/cifar-10\n",
    "\n",
    ":numref:`fig_kaggle_cifar10` shows the information on the competition's webpage. In order to submit the results, please register an account on the Kaggle website first.\n",
    "\n",
    "![CIFAR-10 image classification competition webpage information. The dataset for the competition can be accessed by clicking the \"Data\" tab.](../img/kaggle_cifar10.png)\n",
    ":width:`600px`\n",
    ":label:`fig_kaggle_cifar10`\n",
    "\n",
    "First, import the packages or modules required for the competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "origin_pos": 1,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from d2l import mxnet as d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, npx\n",
    "from mxnet.gluon import nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "time_zero = time.time()\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "## Obtaining and Organizing the Dataset\n",
    "\n",
    "The competition data is divided into a training set and testing set. The training set contains $50,000$ images. The testing set contains $300,000$ images, of which $10,000$ images are used for scoring, while the other $290,000$ non-scoring images are included to prevent the manual labeling of the testing set and the submission of labeling results. The image formats in both datasets are PNG, with heights and widths of 32 pixels and three color channels (RGB). The images cover $10$ categories: planes, cars, birds, cats, deer, dogs, frogs, horses, boats, and trucks. The upper-left corner of Figure 9.16 shows some images of planes, cars, and birds in the dataset.\n",
    "\n",
    "### Downloading the Dataset\n",
    "\n",
    "After logging in to Kaggle, we can click on the \"Data\" tab on the CIFAR-10 image classification competition webpage shown in :numref:`fig_kaggle_cifar10` and download the dataset by clicking the \"Download All\" button. After unzipping the downloaded file in `../data`, and unzipping `train.7z` and `test.7z` inside it, you will find the entire dataset in the following paths:\n",
    "\n",
    "* ../data/cifar-10/train/[1-50000].png\n",
    "* ../data/cifar-10/test/[1-300000].png\n",
    "* ../data/cifar-10/trainLabels.csv\n",
    "* ../data/cifar-10/sampleSubmission.csv\n",
    "\n",
    "Here folders `train` and `test` contain the training and testing images respectively, `trainLabels.csv` has labels for the training images, and `sample_submission.csv` is a sample of submission. \n",
    "\n",
    "To make it easier to get started, we provide a small-scale sample of the dataset: it contains the first $1000$ training images and $5$ random testing images.\n",
    "To use the full dataset of the Kaggle competition, you need to set the following `demo` variable to `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "origin_pos": 3,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',\n",
    "                                '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')\n",
    "\n",
    "# If you use the full dataset downloaded for the Kaggle competition, set\n",
    "# `demo` to False\n",
    "demo = False\n",
    "\n",
    "if demo:\n",
    "    data_dir = d2l.download_extract('cifar10_tiny')\n",
    "else:\n",
    "    data_dir = '../data/cifar-10/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "### Organizing the Dataset\n",
    "\n",
    "We need to organize datasets to facilitate model training and testing. Let us first read the labels from the csv file. The following function returns a dictionary that maps the filename without extension to its label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "origin_pos": 5,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training examples: 50000\n",
      "# classes: 10\n"
     ]
    }
   ],
   "source": [
    "#@save\n",
    "def read_csv_labels(fname):\n",
    "    \"\"\"Read fname to return a name to label dictionary.\"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        # Skip the file header line (column name)\n",
    "        lines = f.readlines()[1:]\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    return dict(((name, label) for name, label in tokens))\n",
    "\n",
    "labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "print('# training examples:', len(labels))\n",
    "print('# classes:', len(set(labels.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "Next, we define the `reorg_train_valid` function to segment the validation set from the original training set. The argument `valid_ratio` in this function is the ratio of the number of examples in the validation set to the number of examples in the original training set. In particular, let $n$ be the number of images of the class with the least examples, and $r$ be the ratio, then we will use $\\max(\\lfloor nr\\rfloor,1)$ images for each class as the validation set.  Let us use `valid_ratio=0.1` as an example. Since the original training set has $50,000$ images, there will be $45,000$ images used for training and stored in the path \"`train_valid_test/train`\" when tuning hyperparameters, while the other $5,000$ images will be stored as validation set in the path \"`train_valid_test/valid`\". After organizing the data, images of the same class will be placed under the same folder so that we can read them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "origin_pos": 7,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def copyfile(filename, target_dir):\n",
    "    \"\"\"Copy a file into a target directory.\"\"\"\n",
    "    d2l.mkdir_if_not_exist(target_dir)\n",
    "    shutil.copy(filename, target_dir)\n",
    "\n",
    "#@save    \n",
    "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
    "    # The number of examples of the class with the least examples in the\n",
    "    # training dataset\n",
    "    n = collections.Counter(labels.values()).most_common()[-1][1]\n",
    "    # The number of examples per class for the validation set\n",
    "    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n",
    "        label = labels[train_file.split('.')[0]]\n",
    "        fname = os.path.join(data_dir, 'train', train_file)\n",
    "        # Copy to train_valid_test/train_valid with a subfolder per class\n",
    "        copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                     'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            # Copy to train_valid_test/valid\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                         'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            # Copy to train_valid_test/train\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                         'train', label))\n",
    "    return n_valid_per_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "The `reorg_test` function below is used to organize the testing set to facilitate the reading during prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "origin_pos": 9,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "#@save    \n",
    "def reorg_test(data_dir):\n",
    "    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n",
    "        copyfile(os.path.join(data_dir, 'test', test_file),\n",
    "                 os.path.join(data_dir, 'train_valid_test', 'test',\n",
    "                              'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Finally, we use a function to call the previously defined `read_csv_labels`, `reorg_train_valid`, and `reorg_test` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "origin_pos": 11,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def reorg_cifar10_data(data_dir, valid_ratio):\n",
    "    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "    reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    reorg_test(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "We only set the batch size to $1$ for the demo dataset. During actual training and testing, the complete dataset of the Kaggle competition should be used and `batch_size` should be set to a larger integer, such as $128$. We use $10\\%$ of the training examples as the validation set for tuning hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "origin_pos": 13,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 1 if demo else 128\n",
    "valid_ratio = 0.1\n",
    "reorg_cifar10_data(data_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## Image Augmentation\n",
    "\n",
    "To cope with overfitting, we use image augmentation. For example, by adding `transforms.RandomFlipLeftRight()`, the images can be flipped at random. We can also perform normalization for the three RGB channels of color images using `transforms.Normalize()`. Below, we list some of these operations that you can choose to use or modify depending on requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "origin_pos": 15,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "transform_train = gluon.data.vision.transforms.Compose([\n",
    "    # Magnify the image to a square of 40 pixels in both height and width\n",
    "    gluon.data.vision.transforms.Resize(40),\n",
    "    # Randomly crop a square image of 40 pixels in both height and width to\n",
    "    # produce a small square of 0.64 to 1 times the area of the original\n",
    "    # image, and then shrink it to a square of 32 pixels in both height and\n",
    "    # width\n",
    "    gluon.data.vision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "                                                   ratio=(1.0, 1.0)),\n",
    "    gluon.data.vision.transforms.RandomFlipLeftRight(),\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    # Normalize each channel of the image\n",
    "    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                           [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "In order to ensure the certainty of the output during testing, we only perform normalization on the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "transform_test = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                           [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Next, we can create the `ImageFolderDataset` instance to read the organized dataset containing the original image files, where each example includes the image and label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "origin_pos": 19,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/d2l/lib/python3.7/site-packages/mxnet/gluon/data/vision/datasets.py:318: UserWarning: Ignoring ../data/cifar-10/train_valid_test/test/unknown/test.7z of type .7z. Only support .jpg, .jpeg, .png\n",
      "  filename, ext, ', '.join(self._exts)))\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, train_valid_ds, test_ds = [\n",
    "    gluon.data.vision.ImageFolderDataset(\n",
    "        os.path.join(data_dir, 'train_valid_test', folder))\n",
    "    for folder in ['train', 'valid', 'train_valid', 'test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "We specify the defined image augmentation operation in `DataLoader`. During training, we only use the validation set to evaluate the model, so we need to ensure the certainty of the output. During prediction, we will train the model on the combined training set and validation set to make full use of all labelled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [gluon.data.DataLoader(\n",
    "    dataset.transform_first(transform_train), batch_size, shuffle=True, \n",
    "    last_batch='keep') for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter, test_iter = [gluon.data.DataLoader(\n",
    "    dataset.transform_first(transform_test), batch_size, shuffle=False, \n",
    "    last_batch='keep') for dataset in (valid_ds, test_ds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "Here, we build the residual blocks based on the `HybridBlock` class, which is\n",
    "slightly different than the implementation described in\n",
    ":numref:`sec_resnet`. This is done to improve execution efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "origin_pos": 23,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n",
    "                               strides=strides)\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\n",
    "                                   strides=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "\n",
    "    def hybrid_forward(self, F, X):\n",
    "        Y = F.npx.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.npx.relu(Y + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "Next, we define the ResNet-18 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def resnet18(num_classes):\n",
    "    net = nn.HybridSequential()\n",
    "    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n",
    "            nn.BatchNorm(), nn.Activation('relu'))\n",
    "\n",
    "    def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "        blk = nn.HybridSequential()\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.add(Residual(num_channels))\n",
    "        return blk\n",
    "\n",
    "    net.add(resnet_block(64, 2, first_block=True),\n",
    "            resnet_block(128, 2),\n",
    "            resnet_block(256, 2),\n",
    "            resnet_block(512, 2))\n",
    "    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "The CIFAR-10 image classification challenge uses 10 categories. We will perform Xavier random initialization on the model before training begins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def get_net(ctx):\n",
    "    num_classes = 10\n",
    "    net = resnet18(num_classes)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## Defining the Training Functions\n",
    "\n",
    "We will select the model and tune hyperparameters according to the model's performance on the validation set. Next, we define the model training function `train`. We record the training time of each epoch, which helps us compare the time costs of different models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "origin_pos": 29,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in train_iter:\n",
    "            y = y.astype('float32').as_in_ctx(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X.as_in_ctx(ctx))\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += float(l)\n",
    "            train_acc_sum += float(\n",
    "                (y_hat.argmax(axis=1).astype(y.dtype) == y).sum())\n",
    "            n += y.size\n",
    "        time_s = f'time {time.time() - start:.2f} sec'\n",
    "        if valid_iter is not None:\n",
    "            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)\n",
    "            epoch_s = (f'epoch {epoch + 1}, loss {train_l_sum / n:f}, '\n",
    "                       f'train acc {train_acc_sum / n:f}, '\n",
    "                       f'valid acc {valid_acc:f}, ')\n",
    "        else:\n",
    "            epoch_s = (f'epoch {epoch + 1}, loss {train_l_sum / n:f}, '\n",
    "                       f'train acc {train_acc_sum / n:f}, ')\n",
    "        print(epoch_s + time_s + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## Training and Validating the Model\n",
    "\n",
    "Now, we can train and validate the model. The following hyperparameters can be tuned. For example, we can increase the number of epochs. Because `lr_period` and `lr_decay` are set to 80 and 0.1 respectively, the learning rate of the optimization algorithm will be multiplied by 0.1 after every 80 epochs. For simplicity, we only train one epoch here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "origin_pos": 31,
    "scrolled": false,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.262936, train acc 0.204533, valid acc 0.328800, time 56.77 sec, lr 0.1\n",
      "epoch 2, loss 1.662022, train acc 0.387867, valid acc 0.456400, time 54.75 sec, lr 0.1\n",
      "epoch 3, loss 1.479550, train acc 0.456178, valid acc 0.494400, time 54.36 sec, lr 0.1\n",
      "epoch 4, loss 1.305951, train acc 0.528733, valid acc 0.532000, time 54.65 sec, lr 0.1\n",
      "epoch 5, loss 1.140732, train acc 0.591289, valid acc 0.589000, time 54.84 sec, lr 0.1\n",
      "epoch 6, loss 0.971415, train acc 0.657333, valid acc 0.633800, time 54.56 sec, lr 0.1\n",
      "epoch 7, loss 0.830940, train acc 0.706533, valid acc 0.674400, time 54.60 sec, lr 0.1\n",
      "epoch 8, loss 0.727819, train acc 0.745844, valid acc 0.688800, time 54.40 sec, lr 0.1\n",
      "epoch 9, loss 0.651817, train acc 0.774444, valid acc 0.743800, time 54.93 sec, lr 0.1\n",
      "epoch 10, loss 0.608638, train acc 0.789733, valid acc 0.756600, time 55.66 sec, lr 0.1\n",
      "epoch 11, loss 0.581556, train acc 0.798622, valid acc 0.772200, time 54.55 sec, lr 0.1\n",
      "epoch 12, loss 0.544407, train acc 0.810156, valid acc 0.581400, time 55.12 sec, lr 0.1\n",
      "epoch 13, loss 0.523028, train acc 0.818533, valid acc 0.775600, time 55.44 sec, lr 0.1\n",
      "epoch 14, loss 0.499649, train acc 0.827467, valid acc 0.792000, time 58.62 sec, lr 0.1\n",
      "epoch 15, loss 0.484385, train acc 0.833267, valid acc 0.784000, time 57.46 sec, lr 0.1\n",
      "epoch 16, loss 0.468043, train acc 0.839111, valid acc 0.697400, time 56.40 sec, lr 0.1\n",
      "epoch 17, loss 0.463609, train acc 0.838978, valid acc 0.761200, time 55.94 sec, lr 0.1\n",
      "epoch 18, loss 0.446036, train acc 0.845444, valid acc 0.761000, time 54.63 sec, lr 0.1\n",
      "epoch 19, loss 0.442451, train acc 0.847444, valid acc 0.802800, time 54.06 sec, lr 0.1\n",
      "epoch 20, loss 0.429901, train acc 0.852044, valid acc 0.785000, time 55.11 sec, lr 0.1\n",
      "epoch 21, loss 0.421716, train acc 0.853756, valid acc 0.713000, time 54.91 sec, lr 0.1\n",
      "epoch 22, loss 0.420321, train acc 0.855556, valid acc 0.761200, time 54.68 sec, lr 0.1\n",
      "epoch 23, loss 0.417014, train acc 0.857978, valid acc 0.812800, time 54.76 sec, lr 0.1\n",
      "epoch 24, loss 0.411328, train acc 0.857289, valid acc 0.817400, time 54.66 sec, lr 0.1\n",
      "epoch 25, loss 0.403064, train acc 0.862667, valid acc 0.706600, time 54.80 sec, lr 0.1\n",
      "epoch 26, loss 0.398597, train acc 0.863089, valid acc 0.683600, time 54.49 sec, lr 0.1\n",
      "epoch 27, loss 0.397099, train acc 0.862911, valid acc 0.773600, time 55.25 sec, lr 0.1\n",
      "epoch 28, loss 0.382435, train acc 0.867711, valid acc 0.759800, time 54.79 sec, lr 0.1\n",
      "epoch 29, loss 0.385728, train acc 0.869600, valid acc 0.811400, time 54.61 sec, lr 0.1\n",
      "epoch 30, loss 0.382737, train acc 0.868867, valid acc 0.818200, time 54.35 sec, lr 0.1\n",
      "epoch 31, loss 0.383629, train acc 0.868200, valid acc 0.686800, time 54.55 sec, lr 0.1\n",
      "epoch 32, loss 0.377854, train acc 0.869933, valid acc 0.787600, time 54.81 sec, lr 0.1\n",
      "epoch 33, loss 0.370379, train acc 0.871889, valid acc 0.762400, time 54.84 sec, lr 0.1\n",
      "epoch 34, loss 0.371743, train acc 0.872911, valid acc 0.816800, time 54.18 sec, lr 0.1\n",
      "epoch 35, loss 0.370419, train acc 0.870800, valid acc 0.783800, time 54.80 sec, lr 0.1\n",
      "epoch 36, loss 0.368053, train acc 0.873133, valid acc 0.773400, time 54.59 sec, lr 0.1\n",
      "epoch 37, loss 0.358762, train acc 0.876133, valid acc 0.807200, time 54.84 sec, lr 0.1\n",
      "epoch 38, loss 0.360519, train acc 0.875400, valid acc 0.813600, time 54.67 sec, lr 0.1\n",
      "epoch 39, loss 0.365584, train acc 0.873133, valid acc 0.813400, time 54.58 sec, lr 0.1\n",
      "epoch 40, loss 0.356630, train acc 0.876778, valid acc 0.797600, time 54.69 sec, lr 0.1\n",
      "epoch 41, loss 0.351478, train acc 0.879867, valid acc 0.800000, time 55.14 sec, lr 0.1\n",
      "epoch 42, loss 0.352943, train acc 0.879822, valid acc 0.805200, time 54.91 sec, lr 0.1\n",
      "epoch 43, loss 0.356694, train acc 0.878467, valid acc 0.810400, time 54.15 sec, lr 0.1\n",
      "epoch 44, loss 0.354649, train acc 0.877311, valid acc 0.804400, time 54.67 sec, lr 0.1\n",
      "epoch 45, loss 0.348496, train acc 0.880333, valid acc 0.813600, time 54.69 sec, lr 0.1\n",
      "epoch 46, loss 0.354385, train acc 0.877400, valid acc 0.801800, time 55.40 sec, lr 0.1\n",
      "epoch 47, loss 0.348551, train acc 0.880844, valid acc 0.812800, time 54.28 sec, lr 0.1\n",
      "epoch 48, loss 0.342878, train acc 0.881289, valid acc 0.762200, time 54.14 sec, lr 0.1\n",
      "epoch 49, loss 0.342929, train acc 0.882556, valid acc 0.709200, time 55.30 sec, lr 0.1\n",
      "epoch 50, loss 0.337651, train acc 0.883444, valid acc 0.821400, time 54.25 sec, lr 0.1\n",
      "epoch 51, loss 0.341216, train acc 0.882311, valid acc 0.718000, time 54.40 sec, lr 0.1\n",
      "epoch 52, loss 0.338146, train acc 0.885400, valid acc 0.814000, time 54.51 sec, lr 0.1\n",
      "epoch 53, loss 0.347643, train acc 0.881067, valid acc 0.829600, time 54.56 sec, lr 0.1\n",
      "epoch 54, loss 0.330774, train acc 0.885422, valid acc 0.816400, time 55.08 sec, lr 0.1\n",
      "epoch 55, loss 0.343927, train acc 0.881844, valid acc 0.829200, time 54.29 sec, lr 0.1\n",
      "epoch 56, loss 0.334318, train acc 0.883200, valid acc 0.789800, time 55.32 sec, lr 0.1\n",
      "epoch 57, loss 0.332304, train acc 0.885578, valid acc 0.802000, time 54.58 sec, lr 0.1\n",
      "epoch 58, loss 0.335656, train acc 0.884089, valid acc 0.755600, time 54.33 sec, lr 0.1\n",
      "epoch 59, loss 0.346358, train acc 0.881822, valid acc 0.731200, time 55.29 sec, lr 0.1\n",
      "epoch 60, loss 0.331903, train acc 0.885978, valid acc 0.774800, time 54.86 sec, lr 0.1\n",
      "epoch 61, loss 0.338240, train acc 0.882511, valid acc 0.778600, time 54.31 sec, lr 0.1\n",
      "epoch 62, loss 0.340704, train acc 0.882578, valid acc 0.755200, time 54.33 sec, lr 0.1\n",
      "epoch 63, loss 0.332818, train acc 0.886689, valid acc 0.843800, time 54.36 sec, lr 0.1\n",
      "epoch 64, loss 0.332013, train acc 0.887311, valid acc 0.816800, time 54.62 sec, lr 0.1\n",
      "epoch 65, loss 0.330601, train acc 0.886289, valid acc 0.808200, time 55.39 sec, lr 0.1\n",
      "epoch 66, loss 0.331724, train acc 0.886733, valid acc 0.828200, time 54.54 sec, lr 0.1\n",
      "epoch 67, loss 0.332640, train acc 0.886289, valid acc 0.822600, time 54.73 sec, lr 0.1\n",
      "epoch 68, loss 0.327174, train acc 0.888089, valid acc 0.786000, time 54.46 sec, lr 0.1\n",
      "epoch 69, loss 0.327595, train acc 0.888356, valid acc 0.650600, time 55.00 sec, lr 0.1\n",
      "epoch 70, loss 0.332812, train acc 0.885511, valid acc 0.782800, time 54.94 sec, lr 0.1\n",
      "epoch 71, loss 0.321789, train acc 0.891111, valid acc 0.795400, time 54.52 sec, lr 0.1\n",
      "epoch 72, loss 0.328083, train acc 0.886778, valid acc 0.779800, time 55.29 sec, lr 0.1\n",
      "epoch 73, loss 0.324720, train acc 0.888267, valid acc 0.815800, time 54.60 sec, lr 0.1\n",
      "epoch 74, loss 0.325683, train acc 0.888667, valid acc 0.784600, time 55.19 sec, lr 0.1\n",
      "epoch 75, loss 0.325930, train acc 0.890000, valid acc 0.691000, time 54.98 sec, lr 0.1\n",
      "epoch 76, loss 0.327366, train acc 0.887822, valid acc 0.847800, time 53.94 sec, lr 0.1\n",
      "epoch 77, loss 0.327633, train acc 0.886667, valid acc 0.759000, time 55.40 sec, lr 0.1\n",
      "epoch 78, loss 0.320989, train acc 0.889444, valid acc 0.747400, time 54.89 sec, lr 0.1\n",
      "epoch 79, loss 0.322442, train acc 0.889667, valid acc 0.808600, time 54.69 sec, lr 0.1\n",
      "epoch 80, loss 0.330866, train acc 0.886089, valid acc 0.828600, time 55.46 sec, lr 0.1\n",
      "epoch 81, loss 0.200125, train acc 0.933444, valid acc 0.894800, time 54.49 sec, lr 0.010000000000000002\n",
      "epoch 82, loss 0.127286, train acc 0.957933, valid acc 0.909600, time 55.39 sec, lr 0.010000000000000002\n",
      "epoch 83, loss 0.105757, train acc 0.965933, valid acc 0.913000, time 55.05 sec, lr 0.010000000000000002\n",
      "epoch 84, loss 0.090905, train acc 0.970267, valid acc 0.914200, time 54.91 sec, lr 0.010000000000000002\n",
      "epoch 85, loss 0.082195, train acc 0.974267, valid acc 0.911000, time 54.41 sec, lr 0.010000000000000002\n",
      "epoch 86, loss 0.073600, train acc 0.977133, valid acc 0.914200, time 54.77 sec, lr 0.010000000000000002\n",
      "epoch 87, loss 0.064519, train acc 0.978933, valid acc 0.918800, time 54.86 sec, lr 0.010000000000000002\n",
      "epoch 88, loss 0.060024, train acc 0.980644, valid acc 0.921400, time 54.81 sec, lr 0.010000000000000002\n",
      "epoch 89, loss 0.054232, train acc 0.982800, valid acc 0.913800, time 54.65 sec, lr 0.010000000000000002\n",
      "epoch 90, loss 0.048813, train acc 0.985311, valid acc 0.914600, time 54.95 sec, lr 0.010000000000000002\n",
      "epoch 91, loss 0.047635, train acc 0.985044, valid acc 0.915800, time 55.09 sec, lr 0.010000000000000002\n",
      "epoch 92, loss 0.044867, train acc 0.986667, valid acc 0.911800, time 54.68 sec, lr 0.010000000000000002\n",
      "epoch 93, loss 0.039309, train acc 0.988378, valid acc 0.916400, time 55.07 sec, lr 0.010000000000000002\n",
      "epoch 94, loss 0.037895, train acc 0.988844, valid acc 0.913400, time 54.40 sec, lr 0.010000000000000002\n",
      "epoch 95, loss 0.036764, train acc 0.988889, valid acc 0.917200, time 54.82 sec, lr 0.010000000000000002\n",
      "epoch 96, loss 0.036382, train acc 0.988978, valid acc 0.914000, time 54.18 sec, lr 0.010000000000000002\n",
      "epoch 97, loss 0.035326, train acc 0.988844, valid acc 0.914000, time 54.78 sec, lr 0.010000000000000002\n",
      "epoch 98, loss 0.032444, train acc 0.990444, valid acc 0.919400, time 54.40 sec, lr 0.010000000000000002\n",
      "epoch 99, loss 0.034999, train acc 0.989244, valid acc 0.922800, time 54.65 sec, lr 0.010000000000000002\n",
      "epoch 100, loss 0.032787, train acc 0.989644, valid acc 0.917200, time 54.06 sec, lr 0.010000000000000002\n",
      "epoch 101, loss 0.029941, train acc 0.990956, valid acc 0.915200, time 54.87 sec, lr 0.010000000000000002\n",
      "epoch 102, loss 0.030259, train acc 0.991133, valid acc 0.912600, time 54.69 sec, lr 0.010000000000000002\n",
      "epoch 103, loss 0.029789, train acc 0.991467, valid acc 0.914600, time 54.60 sec, lr 0.010000000000000002\n",
      "epoch 104, loss 0.030076, train acc 0.990689, valid acc 0.917000, time 55.24 sec, lr 0.010000000000000002\n",
      "epoch 105, loss 0.030931, train acc 0.990600, valid acc 0.916200, time 54.71 sec, lr 0.010000000000000002\n",
      "epoch 106, loss 0.032601, train acc 0.989978, valid acc 0.923200, time 54.91 sec, lr 0.010000000000000002\n",
      "epoch 107, loss 0.031352, train acc 0.990311, valid acc 0.918600, time 54.35 sec, lr 0.010000000000000002\n",
      "epoch 108, loss 0.030015, train acc 0.990867, valid acc 0.918200, time 54.81 sec, lr 0.010000000000000002\n",
      "epoch 109, loss 0.029678, train acc 0.991067, valid acc 0.921200, time 55.04 sec, lr 0.010000000000000002\n",
      "epoch 110, loss 0.031944, train acc 0.990133, valid acc 0.920000, time 54.67 sec, lr 0.010000000000000002\n",
      "epoch 111, loss 0.033987, train acc 0.989400, valid acc 0.906600, time 54.69 sec, lr 0.010000000000000002\n",
      "epoch 112, loss 0.033803, train acc 0.989578, valid acc 0.911600, time 54.72 sec, lr 0.010000000000000002\n",
      "epoch 113, loss 0.032185, train acc 0.990556, valid acc 0.902600, time 54.95 sec, lr 0.010000000000000002\n",
      "epoch 114, loss 0.031420, train acc 0.990067, valid acc 0.908800, time 55.06 sec, lr 0.010000000000000002\n",
      "epoch 115, loss 0.038705, train acc 0.987489, valid acc 0.912200, time 54.69 sec, lr 0.010000000000000002\n",
      "epoch 116, loss 0.033754, train acc 0.989844, valid acc 0.907400, time 54.74 sec, lr 0.010000000000000002\n",
      "epoch 117, loss 0.038154, train acc 0.987511, valid acc 0.915800, time 54.96 sec, lr 0.010000000000000002\n",
      "epoch 118, loss 0.036662, train acc 0.988600, valid acc 0.917200, time 54.78 sec, lr 0.010000000000000002\n",
      "epoch 119, loss 0.040877, train acc 0.987111, valid acc 0.916600, time 54.99 sec, lr 0.010000000000000002\n",
      "epoch 120, loss 0.039777, train acc 0.987489, valid acc 0.907000, time 54.57 sec, lr 0.010000000000000002\n",
      "epoch 121, loss 0.043876, train acc 0.986111, valid acc 0.911400, time 54.89 sec, lr 0.010000000000000002\n",
      "epoch 122, loss 0.041536, train acc 0.987000, valid acc 0.913600, time 54.47 sec, lr 0.010000000000000002\n",
      "epoch 123, loss 0.041264, train acc 0.987244, valid acc 0.903000, time 55.52 sec, lr 0.010000000000000002\n",
      "epoch 124, loss 0.041962, train acc 0.986822, valid acc 0.909600, time 54.34 sec, lr 0.010000000000000002\n",
      "epoch 125, loss 0.045482, train acc 0.985067, valid acc 0.915200, time 55.26 sec, lr 0.010000000000000002\n",
      "epoch 126, loss 0.044796, train acc 0.985400, valid acc 0.913400, time 54.75 sec, lr 0.010000000000000002\n",
      "epoch 127, loss 0.049555, train acc 0.983489, valid acc 0.896200, time 54.33 sec, lr 0.010000000000000002\n",
      "epoch 128, loss 0.049583, train acc 0.984222, valid acc 0.911200, time 54.28 sec, lr 0.010000000000000002\n",
      "epoch 129, loss 0.045279, train acc 0.985333, valid acc 0.893000, time 55.12 sec, lr 0.010000000000000002\n",
      "epoch 130, loss 0.048070, train acc 0.984089, valid acc 0.918000, time 54.19 sec, lr 0.010000000000000002\n",
      "epoch 131, loss 0.043256, train acc 0.986356, valid acc 0.895600, time 54.55 sec, lr 0.010000000000000002\n",
      "epoch 132, loss 0.050097, train acc 0.984178, valid acc 0.918600, time 55.23 sec, lr 0.010000000000000002\n",
      "epoch 133, loss 0.047649, train acc 0.984311, valid acc 0.903400, time 54.45 sec, lr 0.010000000000000002\n",
      "epoch 134, loss 0.043733, train acc 0.986200, valid acc 0.900400, time 55.08 sec, lr 0.010000000000000002\n",
      "epoch 135, loss 0.044304, train acc 0.985733, valid acc 0.888200, time 55.01 sec, lr 0.010000000000000002\n",
      "epoch 136, loss 0.051518, train acc 0.983022, valid acc 0.892000, time 54.46 sec, lr 0.010000000000000002\n",
      "epoch 137, loss 0.048662, train acc 0.984533, valid acc 0.908000, time 54.78 sec, lr 0.010000000000000002\n",
      "epoch 138, loss 0.050936, train acc 0.983467, valid acc 0.913800, time 55.30 sec, lr 0.010000000000000002\n",
      "epoch 139, loss 0.047673, train acc 0.984644, valid acc 0.903400, time 54.49 sec, lr 0.010000000000000002\n",
      "epoch 140, loss 0.045432, train acc 0.985844, valid acc 0.899600, time 54.69 sec, lr 0.010000000000000002\n",
      "epoch 141, loss 0.044544, train acc 0.985644, valid acc 0.895000, time 54.21 sec, lr 0.010000000000000002\n",
      "epoch 142, loss 0.047975, train acc 0.983956, valid acc 0.897400, time 54.94 sec, lr 0.010000000000000002\n",
      "epoch 143, loss 0.046354, train acc 0.985289, valid acc 0.909200, time 54.55 sec, lr 0.010000000000000002\n",
      "epoch 144, loss 0.050863, train acc 0.983356, valid acc 0.913600, time 55.05 sec, lr 0.010000000000000002\n",
      "epoch 145, loss 0.050343, train acc 0.983378, valid acc 0.909800, time 54.77 sec, lr 0.010000000000000002\n",
      "epoch 146, loss 0.046689, train acc 0.985089, valid acc 0.902600, time 55.08 sec, lr 0.010000000000000002\n",
      "epoch 147, loss 0.048458, train acc 0.984178, valid acc 0.917000, time 54.01 sec, lr 0.010000000000000002\n",
      "epoch 148, loss 0.048086, train acc 0.984133, valid acc 0.896600, time 54.67 sec, lr 0.010000000000000002\n",
      "epoch 149, loss 0.047692, train acc 0.984289, valid acc 0.905000, time 54.38 sec, lr 0.010000000000000002\n",
      "epoch 150, loss 0.050355, train acc 0.983289, valid acc 0.907800, time 54.68 sec, lr 0.010000000000000002\n",
      "epoch 151, loss 0.050726, train acc 0.983333, valid acc 0.911200, time 54.58 sec, lr 0.010000000000000002\n",
      "epoch 152, loss 0.049457, train acc 0.983556, valid acc 0.909800, time 54.09 sec, lr 0.010000000000000002\n",
      "epoch 153, loss 0.047603, train acc 0.985222, valid acc 0.909400, time 54.76 sec, lr 0.010000000000000002\n",
      "epoch 154, loss 0.046082, train acc 0.985289, valid acc 0.901600, time 54.45 sec, lr 0.010000000000000002\n",
      "epoch 155, loss 0.048875, train acc 0.984022, valid acc 0.898400, time 54.67 sec, lr 0.010000000000000002\n",
      "epoch 156, loss 0.044837, train acc 0.985911, valid acc 0.906800, time 54.15 sec, lr 0.010000000000000002\n",
      "epoch 157, loss 0.044064, train acc 0.985933, valid acc 0.916200, time 54.33 sec, lr 0.010000000000000002\n",
      "epoch 158, loss 0.043420, train acc 0.986133, valid acc 0.900600, time 54.37 sec, lr 0.010000000000000002\n",
      "epoch 159, loss 0.042821, train acc 0.986511, valid acc 0.905400, time 54.30 sec, lr 0.010000000000000002\n",
      "epoch 160, loss 0.044120, train acc 0.986089, valid acc 0.909400, time 54.37 sec, lr 0.010000000000000002\n",
      "epoch 161, loss 0.029072, train acc 0.990911, valid acc 0.927800, time 54.87 sec, lr 0.0010000000000000002\n",
      "epoch 162, loss 0.017145, train acc 0.995289, valid acc 0.930600, time 54.91 sec, lr 0.0010000000000000002\n",
      "epoch 163, loss 0.012972, train acc 0.996756, valid acc 0.930000, time 54.01 sec, lr 0.0010000000000000002\n",
      "epoch 164, loss 0.011912, train acc 0.997111, valid acc 0.928200, time 54.68 sec, lr 0.0010000000000000002\n",
      "epoch 165, loss 0.010638, train acc 0.997444, valid acc 0.931600, time 54.08 sec, lr 0.0010000000000000002\n",
      "epoch 166, loss 0.009079, train acc 0.997956, valid acc 0.933000, time 53.76 sec, lr 0.0010000000000000002\n",
      "epoch 167, loss 0.008715, train acc 0.997956, valid acc 0.930000, time 54.27 sec, lr 0.0010000000000000002\n",
      "epoch 168, loss 0.008961, train acc 0.997956, valid acc 0.931800, time 54.72 sec, lr 0.0010000000000000002\n",
      "epoch 169, loss 0.009088, train acc 0.997756, valid acc 0.931000, time 54.71 sec, lr 0.0010000000000000002\n",
      "epoch 170, loss 0.007946, train acc 0.998333, valid acc 0.933000, time 54.56 sec, lr 0.0010000000000000002\n",
      "epoch 171, loss 0.007204, train acc 0.998622, valid acc 0.932800, time 54.61 sec, lr 0.0010000000000000002\n",
      "epoch 172, loss 0.007297, train acc 0.998689, valid acc 0.931000, time 54.64 sec, lr 0.0010000000000000002\n",
      "epoch 173, loss 0.006845, train acc 0.998689, valid acc 0.929800, time 53.79 sec, lr 0.0010000000000000002\n",
      "epoch 174, loss 0.006191, train acc 0.998978, valid acc 0.932800, time 54.30 sec, lr 0.0010000000000000002\n",
      "epoch 175, loss 0.006374, train acc 0.998778, valid acc 0.933800, time 54.49 sec, lr 0.0010000000000000002\n",
      "epoch 176, loss 0.006528, train acc 0.998644, valid acc 0.931800, time 54.84 sec, lr 0.0010000000000000002\n",
      "epoch 177, loss 0.006418, train acc 0.998778, valid acc 0.932400, time 55.12 sec, lr 0.0010000000000000002\n",
      "epoch 178, loss 0.006326, train acc 0.998667, valid acc 0.931600, time 54.48 sec, lr 0.0010000000000000002\n",
      "epoch 179, loss 0.005513, train acc 0.999000, valid acc 0.933600, time 54.80 sec, lr 0.0010000000000000002\n",
      "epoch 180, loss 0.005437, train acc 0.999067, valid acc 0.934000, time 53.94 sec, lr 0.0010000000000000002\n",
      "epoch 181, loss 0.005585, train acc 0.998956, valid acc 0.935400, time 54.05 sec, lr 0.0010000000000000002\n",
      "epoch 182, loss 0.005916, train acc 0.998867, valid acc 0.932000, time 54.39 sec, lr 0.0010000000000000002\n",
      "epoch 183, loss 0.005735, train acc 0.998689, valid acc 0.933000, time 54.62 sec, lr 0.0010000000000000002\n",
      "epoch 184, loss 0.005889, train acc 0.998733, valid acc 0.936200, time 54.44 sec, lr 0.0010000000000000002\n",
      "epoch 185, loss 0.005436, train acc 0.998911, valid acc 0.933600, time 54.52 sec, lr 0.0010000000000000002\n",
      "epoch 186, loss 0.005348, train acc 0.999111, valid acc 0.933000, time 54.56 sec, lr 0.0010000000000000002\n",
      "epoch 187, loss 0.004743, train acc 0.999333, valid acc 0.934200, time 54.05 sec, lr 0.0010000000000000002\n",
      "epoch 188, loss 0.005044, train acc 0.999222, valid acc 0.930200, time 54.96 sec, lr 0.0010000000000000002\n",
      "epoch 189, loss 0.004125, train acc 0.999489, valid acc 0.931400, time 54.40 sec, lr 0.0010000000000000002\n",
      "epoch 190, loss 0.004921, train acc 0.999178, valid acc 0.933400, time 54.26 sec, lr 0.0010000000000000002\n",
      "epoch 191, loss 0.004303, train acc 0.999378, valid acc 0.932400, time 54.79 sec, lr 0.0010000000000000002\n",
      "epoch 192, loss 0.004579, train acc 0.999022, valid acc 0.933000, time 54.62 sec, lr 0.0010000000000000002\n",
      "epoch 193, loss 0.004195, train acc 0.999333, valid acc 0.933200, time 54.62 sec, lr 0.0010000000000000002\n",
      "epoch 194, loss 0.004362, train acc 0.999222, valid acc 0.933000, time 54.56 sec, lr 0.0010000000000000002\n",
      "epoch 195, loss 0.004481, train acc 0.999333, valid acc 0.933000, time 54.00 sec, lr 0.0010000000000000002\n",
      "epoch 196, loss 0.004669, train acc 0.999133, valid acc 0.933600, time 54.90 sec, lr 0.0010000000000000002\n",
      "epoch 197, loss 0.004237, train acc 0.999378, valid acc 0.933200, time 54.54 sec, lr 0.0010000000000000002\n",
      "epoch 198, loss 0.004882, train acc 0.999044, valid acc 0.932600, time 54.29 sec, lr 0.0010000000000000002\n",
      "epoch 199, loss 0.004311, train acc 0.999267, valid acc 0.932000, time 54.69 sec, lr 0.0010000000000000002\n",
      "epoch 200, loss 0.004351, train acc 0.999267, valid acc 0.933400, time 53.97 sec, lr 0.0010000000000000002\n",
      "epoch 201, loss 0.004305, train acc 0.999156, valid acc 0.933200, time 54.25 sec, lr 0.0010000000000000002\n",
      "epoch 202, loss 0.003781, train acc 0.999533, valid acc 0.934000, time 55.02 sec, lr 0.0010000000000000002\n",
      "epoch 203, loss 0.003605, train acc 0.999444, valid acc 0.934000, time 54.78 sec, lr 0.0010000000000000002\n",
      "epoch 204, loss 0.003915, train acc 0.999222, valid acc 0.935600, time 55.33 sec, lr 0.0010000000000000002\n",
      "epoch 205, loss 0.003933, train acc 0.999467, valid acc 0.931600, time 54.90 sec, lr 0.0010000000000000002\n",
      "epoch 206, loss 0.004001, train acc 0.999333, valid acc 0.933600, time 54.79 sec, lr 0.0010000000000000002\n",
      "epoch 207, loss 0.003882, train acc 0.999400, valid acc 0.935400, time 54.45 sec, lr 0.0010000000000000002\n",
      "epoch 208, loss 0.003972, train acc 0.999511, valid acc 0.934800, time 54.48 sec, lr 0.0010000000000000002\n",
      "epoch 209, loss 0.003708, train acc 0.999378, valid acc 0.933800, time 54.22 sec, lr 0.0010000000000000002\n",
      "epoch 210, loss 0.004120, train acc 0.999222, valid acc 0.931400, time 54.46 sec, lr 0.0010000000000000002\n",
      "epoch 211, loss 0.004358, train acc 0.999356, valid acc 0.934600, time 53.95 sec, lr 0.0010000000000000002\n",
      "epoch 212, loss 0.003832, train acc 0.999467, valid acc 0.934600, time 55.04 sec, lr 0.0010000000000000002\n",
      "epoch 213, loss 0.003876, train acc 0.999289, valid acc 0.933600, time 54.61 sec, lr 0.0010000000000000002\n",
      "epoch 214, loss 0.003456, train acc 0.999556, valid acc 0.934600, time 54.81 sec, lr 0.0010000000000000002\n",
      "epoch 215, loss 0.003785, train acc 0.999489, valid acc 0.935200, time 55.18 sec, lr 0.0010000000000000002\n",
      "epoch 216, loss 0.003784, train acc 0.999467, valid acc 0.935400, time 54.59 sec, lr 0.0010000000000000002\n",
      "epoch 217, loss 0.003670, train acc 0.999422, valid acc 0.932800, time 54.43 sec, lr 0.0010000000000000002\n",
      "epoch 218, loss 0.003866, train acc 0.999311, valid acc 0.933600, time 54.36 sec, lr 0.0010000000000000002\n",
      "epoch 219, loss 0.003676, train acc 0.999511, valid acc 0.932400, time 54.70 sec, lr 0.0010000000000000002\n",
      "epoch 220, loss 0.003176, train acc 0.999667, valid acc 0.935000, time 55.27 sec, lr 0.0010000000000000002\n",
      "epoch 221, loss 0.003480, train acc 0.999622, valid acc 0.932600, time 54.48 sec, lr 0.0010000000000000002\n",
      "epoch 222, loss 0.003307, train acc 0.999622, valid acc 0.933600, time 54.08 sec, lr 0.0010000000000000002\n",
      "epoch 223, loss 0.003393, train acc 0.999622, valid acc 0.933400, time 54.67 sec, lr 0.0010000000000000002\n",
      "epoch 224, loss 0.003875, train acc 0.999356, valid acc 0.933000, time 54.67 sec, lr 0.0010000000000000002\n",
      "epoch 225, loss 0.003424, train acc 0.999578, valid acc 0.934000, time 53.89 sec, lr 0.0010000000000000002\n",
      "epoch 226, loss 0.003573, train acc 0.999444, valid acc 0.931600, time 54.51 sec, lr 0.0010000000000000002\n",
      "epoch 227, loss 0.003249, train acc 0.999533, valid acc 0.933400, time 54.92 sec, lr 0.0010000000000000002\n",
      "epoch 228, loss 0.003511, train acc 0.999467, valid acc 0.934400, time 54.16 sec, lr 0.0010000000000000002\n",
      "epoch 229, loss 0.003455, train acc 0.999444, valid acc 0.934200, time 54.53 sec, lr 0.0010000000000000002\n",
      "epoch 230, loss 0.003467, train acc 0.999578, valid acc 0.930800, time 54.50 sec, lr 0.0010000000000000002\n",
      "epoch 231, loss 0.003297, train acc 0.999600, valid acc 0.933200, time 54.33 sec, lr 0.0010000000000000002\n",
      "epoch 232, loss 0.003674, train acc 0.999511, valid acc 0.930600, time 54.53 sec, lr 0.0010000000000000002\n",
      "epoch 233, loss 0.003155, train acc 0.999600, valid acc 0.932000, time 54.54 sec, lr 0.0010000000000000002\n",
      "epoch 234, loss 0.003446, train acc 0.999578, valid acc 0.934200, time 54.25 sec, lr 0.0010000000000000002\n",
      "epoch 235, loss 0.003229, train acc 0.999622, valid acc 0.935000, time 53.96 sec, lr 0.0010000000000000002\n",
      "epoch 236, loss 0.003424, train acc 0.999533, valid acc 0.932600, time 54.43 sec, lr 0.0010000000000000002\n",
      "epoch 237, loss 0.003339, train acc 0.999622, valid acc 0.934000, time 54.34 sec, lr 0.0010000000000000002\n",
      "epoch 238, loss 0.003090, train acc 0.999644, valid acc 0.936000, time 54.08 sec, lr 0.0010000000000000002\n",
      "epoch 239, loss 0.003350, train acc 0.999556, valid acc 0.932200, time 54.70 sec, lr 0.0010000000000000002\n",
      "epoch 240, loss 0.003082, train acc 0.999711, valid acc 0.935200, time 54.66 sec, lr 0.0010000000000000002\n",
      "epoch 241, loss 0.002960, train acc 0.999733, valid acc 0.934400, time 54.53 sec, lr 0.00010000000000000003\n",
      "epoch 242, loss 0.002896, train acc 0.999733, valid acc 0.935200, time 54.58 sec, lr 0.00010000000000000003\n",
      "epoch 243, loss 0.003305, train acc 0.999689, valid acc 0.937000, time 54.53 sec, lr 0.00010000000000000003\n",
      "epoch 244, loss 0.003162, train acc 0.999644, valid acc 0.935000, time 54.07 sec, lr 0.00010000000000000003\n",
      "epoch 245, loss 0.003255, train acc 0.999689, valid acc 0.933800, time 54.18 sec, lr 0.00010000000000000003\n",
      "epoch 246, loss 0.003116, train acc 0.999689, valid acc 0.935000, time 55.07 sec, lr 0.00010000000000000003\n",
      "epoch 247, loss 0.003179, train acc 0.999689, valid acc 0.935800, time 54.17 sec, lr 0.00010000000000000003\n",
      "epoch 248, loss 0.003325, train acc 0.999644, valid acc 0.936600, time 54.43 sec, lr 0.00010000000000000003\n",
      "epoch 249, loss 0.003083, train acc 0.999667, valid acc 0.934800, time 53.99 sec, lr 0.00010000000000000003\n",
      "epoch 250, loss 0.003096, train acc 0.999600, valid acc 0.934200, time 54.52 sec, lr 0.00010000000000000003\n",
      "epoch 251, loss 0.003021, train acc 0.999667, valid acc 0.934800, time 54.84 sec, lr 0.00010000000000000003\n",
      "epoch 252, loss 0.003142, train acc 0.999578, valid acc 0.936400, time 54.75 sec, lr 0.00010000000000000003\n",
      "epoch 253, loss 0.003303, train acc 0.999622, valid acc 0.936000, time 54.49 sec, lr 0.00010000000000000003\n",
      "epoch 254, loss 0.003267, train acc 0.999622, valid acc 0.935200, time 54.74 sec, lr 0.00010000000000000003\n",
      "epoch 255, loss 0.003147, train acc 0.999622, valid acc 0.934000, time 54.80 sec, lr 0.00010000000000000003\n",
      "epoch 256, loss 0.003129, train acc 0.999644, valid acc 0.934200, time 54.46 sec, lr 0.00010000000000000003\n",
      "epoch 257, loss 0.003272, train acc 0.999622, valid acc 0.936800, time 54.81 sec, lr 0.00010000000000000003\n",
      "epoch 258, loss 0.003203, train acc 0.999733, valid acc 0.934600, time 54.93 sec, lr 0.00010000000000000003\n",
      "epoch 259, loss 0.003097, train acc 0.999622, valid acc 0.937200, time 55.15 sec, lr 0.00010000000000000003\n",
      "epoch 260, loss 0.002847, train acc 0.999756, valid acc 0.935200, time 54.50 sec, lr 0.00010000000000000003\n",
      "epoch 261, loss 0.002807, train acc 0.999800, valid acc 0.936600, time 54.63 sec, lr 0.00010000000000000003\n",
      "epoch 262, loss 0.003261, train acc 0.999556, valid acc 0.936200, time 54.32 sec, lr 0.00010000000000000003\n",
      "epoch 263, loss 0.002999, train acc 0.999667, valid acc 0.935800, time 54.99 sec, lr 0.00010000000000000003\n",
      "epoch 264, loss 0.002866, train acc 0.999622, valid acc 0.935600, time 54.70 sec, lr 0.00010000000000000003\n",
      "epoch 265, loss 0.003018, train acc 0.999600, valid acc 0.935400, time 54.31 sec, lr 0.00010000000000000003\n",
      "epoch 266, loss 0.003113, train acc 0.999600, valid acc 0.935000, time 54.25 sec, lr 0.00010000000000000003\n",
      "epoch 267, loss 0.003070, train acc 0.999644, valid acc 0.936200, time 54.37 sec, lr 0.00010000000000000003\n",
      "epoch 268, loss 0.003137, train acc 0.999644, valid acc 0.936400, time 54.08 sec, lr 0.00010000000000000003\n",
      "epoch 269, loss 0.003143, train acc 0.999511, valid acc 0.933600, time 54.19 sec, lr 0.00010000000000000003\n",
      "epoch 270, loss 0.003229, train acc 0.999556, valid acc 0.936800, time 55.17 sec, lr 0.00010000000000000003\n",
      "epoch 271, loss 0.003151, train acc 0.999511, valid acc 0.936800, time 54.44 sec, lr 0.00010000000000000003\n",
      "epoch 272, loss 0.003174, train acc 0.999667, valid acc 0.935600, time 54.48 sec, lr 0.00010000000000000003\n",
      "epoch 273, loss 0.002974, train acc 0.999667, valid acc 0.933600, time 53.94 sec, lr 0.00010000000000000003\n",
      "epoch 274, loss 0.003241, train acc 0.999578, valid acc 0.934800, time 54.69 sec, lr 0.00010000000000000003\n",
      "epoch 275, loss 0.003039, train acc 0.999667, valid acc 0.936200, time 54.27 sec, lr 0.00010000000000000003\n",
      "epoch 276, loss 0.003342, train acc 0.999533, valid acc 0.934000, time 54.46 sec, lr 0.00010000000000000003\n",
      "epoch 277, loss 0.002881, train acc 0.999733, valid acc 0.937200, time 54.68 sec, lr 0.00010000000000000003\n",
      "epoch 278, loss 0.003117, train acc 0.999667, valid acc 0.936400, time 54.99 sec, lr 0.00010000000000000003\n",
      "epoch 279, loss 0.002936, train acc 0.999600, valid acc 0.935600, time 54.69 sec, lr 0.00010000000000000003\n",
      "epoch 280, loss 0.003019, train acc 0.999667, valid acc 0.935600, time 54.50 sec, lr 0.00010000000000000003\n",
      "epoch 281, loss 0.002735, train acc 0.999667, valid acc 0.935000, time 54.85 sec, lr 0.00010000000000000003\n",
      "epoch 282, loss 0.003118, train acc 0.999644, valid acc 0.932800, time 54.25 sec, lr 0.00010000000000000003\n",
      "epoch 283, loss 0.003131, train acc 0.999622, valid acc 0.935400, time 54.27 sec, lr 0.00010000000000000003\n",
      "epoch 284, loss 0.002874, train acc 0.999644, valid acc 0.934800, time 55.07 sec, lr 0.00010000000000000003\n",
      "epoch 285, loss 0.002981, train acc 0.999600, valid acc 0.936400, time 54.41 sec, lr 0.00010000000000000003\n",
      "epoch 286, loss 0.003376, train acc 0.999556, valid acc 0.935200, time 54.77 sec, lr 0.00010000000000000003\n",
      "epoch 287, loss 0.002846, train acc 0.999667, valid acc 0.935800, time 54.07 sec, lr 0.00010000000000000003\n",
      "epoch 288, loss 0.003227, train acc 0.999578, valid acc 0.935800, time 54.05 sec, lr 0.00010000000000000003\n",
      "epoch 289, loss 0.002730, train acc 0.999822, valid acc 0.936400, time 54.34 sec, lr 0.00010000000000000003\n",
      "epoch 290, loss 0.002735, train acc 0.999778, valid acc 0.935400, time 54.27 sec, lr 0.00010000000000000003\n",
      "epoch 291, loss 0.003359, train acc 0.999600, valid acc 0.935000, time 54.22 sec, lr 0.00010000000000000003\n",
      "epoch 292, loss 0.002937, train acc 0.999667, valid acc 0.935200, time 54.35 sec, lr 0.00010000000000000003\n",
      "epoch 293, loss 0.002971, train acc 0.999667, valid acc 0.936000, time 54.53 sec, lr 0.00010000000000000003\n",
      "epoch 294, loss 0.002812, train acc 0.999689, valid acc 0.934800, time 54.59 sec, lr 0.00010000000000000003\n",
      "epoch 295, loss 0.002937, train acc 0.999778, valid acc 0.934800, time 55.04 sec, lr 0.00010000000000000003\n",
      "epoch 296, loss 0.003175, train acc 0.999667, valid acc 0.935000, time 55.12 sec, lr 0.00010000000000000003\n",
      "epoch 297, loss 0.002801, train acc 0.999756, valid acc 0.935800, time 54.83 sec, lr 0.00010000000000000003\n",
      "epoch 298, loss 0.002979, train acc 0.999644, valid acc 0.935400, time 54.44 sec, lr 0.00010000000000000003\n",
      "epoch 299, loss 0.003244, train acc 0.999533, valid acc 0.932600, time 54.61 sec, lr 0.00010000000000000003\n",
      "epoch 300, loss 0.002814, train acc 0.999800, valid acc 0.935400, time 54.46 sec, lr 0.00010000000000000003\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 300, 0.1, 5e-4\n",
    "lr_period, lr_decay, net = 80, 0.1, get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current used time : 16951.625024557114\n"
     ]
    }
   ],
   "source": [
    "time_one = time.time()\n",
    "print(\"Current used time : {}\".format(time_one - time_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "## Classifying the Testing Set and Submitting Results on Kaggle\n",
    "\n",
    "After obtaining a satisfactory model design and hyperparameters, we use all training datasets (including validation sets) to retrain the model and classify the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "origin_pos": 33,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.156161, train acc 0.244520, time 61.95 sec, lr 0.1\n",
      "epoch 2, loss 1.581616, train acc 0.423840, time 61.23 sec, lr 0.1\n",
      "epoch 3, loss 1.368533, train acc 0.502400, time 60.50 sec, lr 0.1\n",
      "epoch 4, loss 1.195740, train acc 0.572520, time 60.38 sec, lr 0.1\n",
      "epoch 5, loss 1.024763, train acc 0.636340, time 60.28 sec, lr 0.1\n",
      "epoch 6, loss 0.869902, train acc 0.696060, time 61.21 sec, lr 0.1\n",
      "epoch 7, loss 0.747121, train acc 0.737620, time 60.69 sec, lr 0.1\n",
      "epoch 8, loss 0.668691, train acc 0.768740, time 61.12 sec, lr 0.1\n",
      "epoch 9, loss 0.611589, train acc 0.789700, time 59.96 sec, lr 0.1\n",
      "epoch 10, loss 0.576985, train acc 0.799220, time 60.68 sec, lr 0.1\n",
      "epoch 11, loss 0.552878, train acc 0.807460, time 60.12 sec, lr 0.1\n",
      "epoch 12, loss 0.526121, train acc 0.819620, time 60.13 sec, lr 0.1\n",
      "epoch 13, loss 0.510848, train acc 0.824040, time 60.58 sec, lr 0.1\n",
      "epoch 14, loss 0.485711, train acc 0.832360, time 60.68 sec, lr 0.1\n",
      "epoch 15, loss 0.474487, train acc 0.834660, time 60.28 sec, lr 0.1\n",
      "epoch 16, loss 0.463492, train acc 0.840360, time 60.69 sec, lr 0.1\n",
      "epoch 17, loss 0.451367, train acc 0.844360, time 60.55 sec, lr 0.1\n",
      "epoch 18, loss 0.444518, train acc 0.845860, time 60.16 sec, lr 0.1\n",
      "epoch 19, loss 0.438578, train acc 0.850160, time 60.21 sec, lr 0.1\n",
      "epoch 20, loss 0.430538, train acc 0.851520, time 60.31 sec, lr 0.1\n",
      "epoch 21, loss 0.425762, train acc 0.853480, time 61.35 sec, lr 0.1\n",
      "epoch 22, loss 0.421434, train acc 0.855780, time 60.91 sec, lr 0.1\n",
      "epoch 23, loss 0.409032, train acc 0.858060, time 61.12 sec, lr 0.1\n",
      "epoch 24, loss 0.411544, train acc 0.858780, time 60.96 sec, lr 0.1\n",
      "epoch 25, loss 0.410682, train acc 0.858320, time 61.05 sec, lr 0.1\n",
      "epoch 26, loss 0.398667, train acc 0.862320, time 60.40 sec, lr 0.1\n",
      "epoch 27, loss 0.394989, train acc 0.863080, time 59.91 sec, lr 0.1\n",
      "epoch 28, loss 0.398077, train acc 0.862940, time 60.34 sec, lr 0.1\n",
      "epoch 29, loss 0.392594, train acc 0.865720, time 60.71 sec, lr 0.1\n",
      "epoch 30, loss 0.392374, train acc 0.864120, time 60.68 sec, lr 0.1\n",
      "epoch 31, loss 0.383607, train acc 0.867580, time 60.36 sec, lr 0.1\n",
      "epoch 32, loss 0.380082, train acc 0.867720, time 60.62 sec, lr 0.1\n",
      "epoch 33, loss 0.384946, train acc 0.867080, time 60.78 sec, lr 0.1\n",
      "epoch 34, loss 0.371482, train acc 0.872820, time 60.83 sec, lr 0.1\n",
      "epoch 35, loss 0.376046, train acc 0.869700, time 60.32 sec, lr 0.1\n",
      "epoch 36, loss 0.377044, train acc 0.870140, time 60.54 sec, lr 0.1\n",
      "epoch 37, loss 0.377774, train acc 0.870740, time 60.69 sec, lr 0.1\n",
      "epoch 38, loss 0.373863, train acc 0.872360, time 60.98 sec, lr 0.1\n",
      "epoch 39, loss 0.372765, train acc 0.870260, time 60.87 sec, lr 0.1\n",
      "epoch 40, loss 0.364026, train acc 0.873800, time 60.43 sec, lr 0.1\n",
      "epoch 41, loss 0.368202, train acc 0.873760, time 60.10 sec, lr 0.1\n",
      "epoch 42, loss 0.364816, train acc 0.873920, time 60.92 sec, lr 0.1\n",
      "epoch 43, loss 0.363183, train acc 0.875800, time 60.70 sec, lr 0.1\n",
      "epoch 44, loss 0.365769, train acc 0.874180, time 60.28 sec, lr 0.1\n",
      "epoch 45, loss 0.360428, train acc 0.875840, time 60.38 sec, lr 0.1\n",
      "epoch 46, loss 0.365180, train acc 0.874360, time 61.06 sec, lr 0.1\n",
      "epoch 47, loss 0.353148, train acc 0.879940, time 60.21 sec, lr 0.1\n",
      "epoch 48, loss 0.360632, train acc 0.877880, time 60.81 sec, lr 0.1\n",
      "epoch 49, loss 0.354774, train acc 0.878720, time 60.40 sec, lr 0.1\n",
      "epoch 50, loss 0.356108, train acc 0.877140, time 61.03 sec, lr 0.1\n",
      "epoch 51, loss 0.355446, train acc 0.878360, time 60.52 sec, lr 0.1\n",
      "epoch 52, loss 0.353651, train acc 0.877680, time 60.87 sec, lr 0.1\n",
      "epoch 53, loss 0.350082, train acc 0.882120, time 59.93 sec, lr 0.1\n",
      "epoch 54, loss 0.355993, train acc 0.875920, time 60.36 sec, lr 0.1\n",
      "epoch 55, loss 0.351519, train acc 0.879340, time 60.24 sec, lr 0.1\n",
      "epoch 56, loss 0.345939, train acc 0.881220, time 59.87 sec, lr 0.1\n",
      "epoch 57, loss 0.349852, train acc 0.881280, time 60.59 sec, lr 0.1\n",
      "epoch 58, loss 0.350065, train acc 0.878040, time 60.43 sec, lr 0.1\n",
      "epoch 59, loss 0.351749, train acc 0.879820, time 61.06 sec, lr 0.1\n",
      "epoch 60, loss 0.342522, train acc 0.882660, time 60.16 sec, lr 0.1\n",
      "epoch 61, loss 0.348256, train acc 0.880700, time 60.44 sec, lr 0.1\n",
      "epoch 62, loss 0.343486, train acc 0.881540, time 60.26 sec, lr 0.1\n",
      "epoch 63, loss 0.343430, train acc 0.881480, time 59.84 sec, lr 0.1\n",
      "epoch 64, loss 0.346315, train acc 0.881420, time 60.77 sec, lr 0.1\n",
      "epoch 65, loss 0.344282, train acc 0.883480, time 60.17 sec, lr 0.1\n",
      "epoch 66, loss 0.342050, train acc 0.882060, time 61.12 sec, lr 0.1\n",
      "epoch 67, loss 0.343514, train acc 0.881420, time 60.36 sec, lr 0.1\n",
      "epoch 68, loss 0.344288, train acc 0.882880, time 60.16 sec, lr 0.1\n",
      "epoch 69, loss 0.341886, train acc 0.882760, time 60.41 sec, lr 0.1\n",
      "epoch 70, loss 0.342487, train acc 0.881080, time 60.22 sec, lr 0.1\n",
      "epoch 71, loss 0.344659, train acc 0.882840, time 60.67 sec, lr 0.1\n",
      "epoch 72, loss 0.338563, train acc 0.883220, time 61.14 sec, lr 0.1\n",
      "epoch 73, loss 0.344300, train acc 0.881420, time 60.53 sec, lr 0.1\n",
      "epoch 74, loss 0.335561, train acc 0.885680, time 60.41 sec, lr 0.1\n",
      "epoch 75, loss 0.337491, train acc 0.884040, time 60.32 sec, lr 0.1\n",
      "epoch 76, loss 0.343728, train acc 0.881260, time 60.07 sec, lr 0.1\n",
      "epoch 77, loss 0.338693, train acc 0.883780, time 60.94 sec, lr 0.1\n",
      "epoch 78, loss 0.333113, train acc 0.885440, time 61.17 sec, lr 0.1\n",
      "epoch 79, loss 0.340456, train acc 0.883040, time 60.57 sec, lr 0.1\n",
      "epoch 80, loss 0.333774, train acc 0.884980, time 60.46 sec, lr 0.1\n",
      "epoch 81, loss 0.205594, train acc 0.931520, time 60.61 sec, lr 0.010000000000000002\n",
      "epoch 82, loss 0.131523, train acc 0.956540, time 60.42 sec, lr 0.010000000000000002\n",
      "epoch 83, loss 0.110184, train acc 0.964360, time 60.66 sec, lr 0.010000000000000002\n",
      "epoch 84, loss 0.099550, train acc 0.967620, time 60.53 sec, lr 0.010000000000000002\n",
      "epoch 85, loss 0.085491, train acc 0.971800, time 60.63 sec, lr 0.010000000000000002\n",
      "epoch 86, loss 0.077725, train acc 0.975240, time 60.81 sec, lr 0.010000000000000002\n",
      "epoch 87, loss 0.068914, train acc 0.977940, time 60.46 sec, lr 0.010000000000000002\n",
      "epoch 88, loss 0.063144, train acc 0.978680, time 60.19 sec, lr 0.010000000000000002\n",
      "epoch 89, loss 0.058262, train acc 0.981620, time 61.20 sec, lr 0.010000000000000002\n",
      "epoch 90, loss 0.055083, train acc 0.982320, time 60.33 sec, lr 0.010000000000000002\n",
      "epoch 91, loss 0.050985, train acc 0.983400, time 60.68 sec, lr 0.010000000000000002\n",
      "epoch 92, loss 0.049247, train acc 0.984740, time 60.11 sec, lr 0.010000000000000002\n",
      "epoch 93, loss 0.047198, train acc 0.984820, time 60.25 sec, lr 0.010000000000000002\n",
      "epoch 94, loss 0.041801, train acc 0.987160, time 61.20 sec, lr 0.010000000000000002\n",
      "epoch 95, loss 0.040914, train acc 0.987100, time 60.16 sec, lr 0.010000000000000002\n",
      "epoch 96, loss 0.039854, train acc 0.987320, time 60.20 sec, lr 0.010000000000000002\n",
      "epoch 97, loss 0.039400, train acc 0.987720, time 60.78 sec, lr 0.010000000000000002\n",
      "epoch 98, loss 0.036786, train acc 0.988360, time 60.73 sec, lr 0.010000000000000002\n",
      "epoch 99, loss 0.036301, train acc 0.989120, time 60.18 sec, lr 0.010000000000000002\n",
      "epoch 100, loss 0.035013, train acc 0.988980, time 60.65 sec, lr 0.010000000000000002\n",
      "epoch 101, loss 0.034996, train acc 0.989060, time 60.43 sec, lr 0.010000000000000002\n",
      "epoch 102, loss 0.036452, train acc 0.988480, time 60.30 sec, lr 0.010000000000000002\n",
      "epoch 103, loss 0.037980, train acc 0.987960, time 60.68 sec, lr 0.010000000000000002\n",
      "epoch 104, loss 0.035382, train acc 0.988800, time 60.19 sec, lr 0.010000000000000002\n",
      "epoch 105, loss 0.034167, train acc 0.989360, time 59.87 sec, lr 0.010000000000000002\n",
      "epoch 106, loss 0.037948, train acc 0.988220, time 60.22 sec, lr 0.010000000000000002\n",
      "epoch 107, loss 0.041458, train acc 0.986800, time 60.25 sec, lr 0.010000000000000002\n",
      "epoch 108, loss 0.037020, train acc 0.988300, time 60.39 sec, lr 0.010000000000000002\n",
      "epoch 109, loss 0.037962, train acc 0.987600, time 60.53 sec, lr 0.010000000000000002\n",
      "epoch 110, loss 0.039431, train acc 0.987700, time 60.64 sec, lr 0.010000000000000002\n",
      "epoch 111, loss 0.039979, train acc 0.987800, time 60.21 sec, lr 0.010000000000000002\n",
      "epoch 112, loss 0.037137, train acc 0.988640, time 61.36 sec, lr 0.010000000000000002\n",
      "epoch 113, loss 0.039836, train acc 0.987120, time 60.79 sec, lr 0.010000000000000002\n",
      "epoch 114, loss 0.045178, train acc 0.985440, time 60.76 sec, lr 0.010000000000000002\n",
      "epoch 115, loss 0.043241, train acc 0.986080, time 60.14 sec, lr 0.010000000000000002\n",
      "epoch 116, loss 0.046068, train acc 0.984960, time 60.55 sec, lr 0.010000000000000002\n",
      "epoch 117, loss 0.045848, train acc 0.985820, time 60.14 sec, lr 0.010000000000000002\n",
      "epoch 118, loss 0.049844, train acc 0.983220, time 59.90 sec, lr 0.010000000000000002\n",
      "epoch 119, loss 0.046880, train acc 0.984900, time 60.91 sec, lr 0.010000000000000002\n",
      "epoch 120, loss 0.052253, train acc 0.982940, time 60.14 sec, lr 0.010000000000000002\n",
      "epoch 121, loss 0.046948, train acc 0.984360, time 60.08 sec, lr 0.010000000000000002\n",
      "epoch 122, loss 0.050027, train acc 0.983800, time 60.66 sec, lr 0.010000000000000002\n",
      "epoch 123, loss 0.043802, train acc 0.986160, time 60.03 sec, lr 0.010000000000000002\n",
      "epoch 124, loss 0.044739, train acc 0.985500, time 60.14 sec, lr 0.010000000000000002\n",
      "epoch 125, loss 0.048429, train acc 0.984500, time 60.50 sec, lr 0.010000000000000002\n",
      "epoch 126, loss 0.055752, train acc 0.982060, time 61.26 sec, lr 0.010000000000000002\n",
      "epoch 127, loss 0.051486, train acc 0.983840, time 60.15 sec, lr 0.010000000000000002\n",
      "epoch 128, loss 0.048919, train acc 0.984220, time 61.20 sec, lr 0.010000000000000002\n",
      "epoch 129, loss 0.049615, train acc 0.984220, time 60.47 sec, lr 0.010000000000000002\n",
      "epoch 130, loss 0.052479, train acc 0.982700, time 61.19 sec, lr 0.010000000000000002\n",
      "epoch 131, loss 0.049398, train acc 0.984260, time 60.14 sec, lr 0.010000000000000002\n",
      "epoch 132, loss 0.054854, train acc 0.982100, time 60.89 sec, lr 0.010000000000000002\n",
      "epoch 133, loss 0.057298, train acc 0.981360, time 60.86 sec, lr 0.010000000000000002\n",
      "epoch 134, loss 0.046161, train acc 0.985380, time 60.72 sec, lr 0.010000000000000002\n",
      "epoch 135, loss 0.056579, train acc 0.981300, time 60.89 sec, lr 0.010000000000000002\n",
      "epoch 136, loss 0.054638, train acc 0.982100, time 60.90 sec, lr 0.010000000000000002\n",
      "epoch 137, loss 0.048191, train acc 0.984440, time 60.63 sec, lr 0.010000000000000002\n",
      "epoch 138, loss 0.051860, train acc 0.983520, time 60.21 sec, lr 0.010000000000000002\n",
      "epoch 139, loss 0.054046, train acc 0.982720, time 60.84 sec, lr 0.010000000000000002\n",
      "epoch 140, loss 0.048816, train acc 0.984720, time 60.44 sec, lr 0.010000000000000002\n",
      "epoch 141, loss 0.053378, train acc 0.982260, time 60.47 sec, lr 0.010000000000000002\n",
      "epoch 142, loss 0.048850, train acc 0.984520, time 60.34 sec, lr 0.010000000000000002\n",
      "epoch 143, loss 0.051867, train acc 0.982640, time 60.75 sec, lr 0.010000000000000002\n",
      "epoch 144, loss 0.051370, train acc 0.983560, time 60.53 sec, lr 0.010000000000000002\n",
      "epoch 145, loss 0.047019, train acc 0.984860, time 60.47 sec, lr 0.010000000000000002\n",
      "epoch 146, loss 0.048822, train acc 0.984660, time 60.60 sec, lr 0.010000000000000002\n",
      "epoch 147, loss 0.053507, train acc 0.982660, time 60.31 sec, lr 0.010000000000000002\n",
      "epoch 148, loss 0.048186, train acc 0.984300, time 60.91 sec, lr 0.010000000000000002\n",
      "epoch 149, loss 0.052823, train acc 0.982380, time 60.95 sec, lr 0.010000000000000002\n",
      "epoch 150, loss 0.056951, train acc 0.980780, time 60.10 sec, lr 0.010000000000000002\n",
      "epoch 151, loss 0.050788, train acc 0.983680, time 60.10 sec, lr 0.010000000000000002\n",
      "epoch 152, loss 0.050352, train acc 0.983880, time 60.35 sec, lr 0.010000000000000002\n",
      "epoch 153, loss 0.053365, train acc 0.982340, time 60.23 sec, lr 0.010000000000000002\n",
      "epoch 154, loss 0.045304, train acc 0.985680, time 60.89 sec, lr 0.010000000000000002\n",
      "epoch 155, loss 0.051341, train acc 0.983420, time 60.54 sec, lr 0.010000000000000002\n",
      "epoch 156, loss 0.049697, train acc 0.983980, time 60.39 sec, lr 0.010000000000000002\n",
      "epoch 157, loss 0.047174, train acc 0.985220, time 60.23 sec, lr 0.010000000000000002\n",
      "epoch 158, loss 0.043882, train acc 0.985920, time 60.78 sec, lr 0.010000000000000002\n",
      "epoch 159, loss 0.053862, train acc 0.982400, time 60.22 sec, lr 0.010000000000000002\n",
      "epoch 160, loss 0.050186, train acc 0.983120, time 60.87 sec, lr 0.010000000000000002\n",
      "epoch 161, loss 0.028186, train acc 0.991260, time 60.42 sec, lr 0.0010000000000000002\n",
      "epoch 162, loss 0.016575, train acc 0.995760, time 60.39 sec, lr 0.0010000000000000002\n",
      "epoch 163, loss 0.014155, train acc 0.996520, time 60.11 sec, lr 0.0010000000000000002\n",
      "epoch 164, loss 0.012384, train acc 0.997020, time 60.71 sec, lr 0.0010000000000000002\n",
      "epoch 165, loss 0.011534, train acc 0.997220, time 60.90 sec, lr 0.0010000000000000002\n",
      "epoch 166, loss 0.010432, train acc 0.997700, time 60.43 sec, lr 0.0010000000000000002\n",
      "epoch 167, loss 0.009363, train acc 0.998000, time 60.33 sec, lr 0.0010000000000000002\n",
      "epoch 168, loss 0.009591, train acc 0.997800, time 60.30 sec, lr 0.0010000000000000002\n",
      "epoch 169, loss 0.009474, train acc 0.997920, time 60.15 sec, lr 0.0010000000000000002\n",
      "epoch 170, loss 0.008763, train acc 0.998140, time 60.85 sec, lr 0.0010000000000000002\n",
      "epoch 171, loss 0.008042, train acc 0.998420, time 61.02 sec, lr 0.0010000000000000002\n",
      "epoch 172, loss 0.007862, train acc 0.998260, time 60.51 sec, lr 0.0010000000000000002\n",
      "epoch 173, loss 0.007139, train acc 0.998500, time 60.72 sec, lr 0.0010000000000000002\n",
      "epoch 174, loss 0.007649, train acc 0.998320, time 59.98 sec, lr 0.0010000000000000002\n",
      "epoch 175, loss 0.007361, train acc 0.998460, time 60.82 sec, lr 0.0010000000000000002\n",
      "epoch 176, loss 0.007097, train acc 0.998460, time 60.67 sec, lr 0.0010000000000000002\n",
      "epoch 177, loss 0.006499, train acc 0.998700, time 59.88 sec, lr 0.0010000000000000002\n",
      "epoch 178, loss 0.006005, train acc 0.998820, time 60.50 sec, lr 0.0010000000000000002\n",
      "epoch 179, loss 0.005985, train acc 0.999000, time 60.46 sec, lr 0.0010000000000000002\n",
      "epoch 180, loss 0.006248, train acc 0.998780, time 60.63 sec, lr 0.0010000000000000002\n",
      "epoch 181, loss 0.005969, train acc 0.998620, time 60.21 sec, lr 0.0010000000000000002\n",
      "epoch 182, loss 0.005954, train acc 0.998820, time 60.52 sec, lr 0.0010000000000000002\n",
      "epoch 183, loss 0.005906, train acc 0.998780, time 60.26 sec, lr 0.0010000000000000002\n",
      "epoch 184, loss 0.005626, train acc 0.998900, time 60.72 sec, lr 0.0010000000000000002\n",
      "epoch 185, loss 0.005627, train acc 0.998920, time 60.81 sec, lr 0.0010000000000000002\n",
      "epoch 186, loss 0.005494, train acc 0.999000, time 60.28 sec, lr 0.0010000000000000002\n",
      "epoch 187, loss 0.005164, train acc 0.998980, time 60.79 sec, lr 0.0010000000000000002\n",
      "epoch 188, loss 0.005615, train acc 0.998840, time 60.66 sec, lr 0.0010000000000000002\n",
      "epoch 189, loss 0.005077, train acc 0.999180, time 60.65 sec, lr 0.0010000000000000002\n",
      "epoch 190, loss 0.005189, train acc 0.999120, time 60.13 sec, lr 0.0010000000000000002\n",
      "epoch 191, loss 0.004952, train acc 0.999060, time 60.65 sec, lr 0.0010000000000000002\n",
      "epoch 192, loss 0.004708, train acc 0.999260, time 60.30 sec, lr 0.0010000000000000002\n",
      "epoch 193, loss 0.004853, train acc 0.999080, time 60.54 sec, lr 0.0010000000000000002\n",
      "epoch 194, loss 0.005297, train acc 0.998920, time 60.53 sec, lr 0.0010000000000000002\n",
      "epoch 195, loss 0.004850, train acc 0.999140, time 60.62 sec, lr 0.0010000000000000002\n",
      "epoch 196, loss 0.004542, train acc 0.999380, time 60.40 sec, lr 0.0010000000000000002\n",
      "epoch 197, loss 0.004514, train acc 0.999260, time 60.46 sec, lr 0.0010000000000000002\n",
      "epoch 198, loss 0.004807, train acc 0.998940, time 60.70 sec, lr 0.0010000000000000002\n",
      "epoch 199, loss 0.004030, train acc 0.999360, time 60.66 sec, lr 0.0010000000000000002\n",
      "epoch 200, loss 0.004503, train acc 0.999080, time 60.15 sec, lr 0.0010000000000000002\n",
      "epoch 201, loss 0.004448, train acc 0.999300, time 60.75 sec, lr 0.0010000000000000002\n",
      "epoch 202, loss 0.004706, train acc 0.999140, time 61.08 sec, lr 0.0010000000000000002\n",
      "epoch 203, loss 0.004405, train acc 0.999240, time 60.63 sec, lr 0.0010000000000000002\n",
      "epoch 204, loss 0.004055, train acc 0.999480, time 60.90 sec, lr 0.0010000000000000002\n",
      "epoch 205, loss 0.004572, train acc 0.999160, time 60.44 sec, lr 0.0010000000000000002\n",
      "epoch 206, loss 0.004387, train acc 0.999200, time 61.29 sec, lr 0.0010000000000000002\n",
      "epoch 207, loss 0.003802, train acc 0.999500, time 60.19 sec, lr 0.0010000000000000002\n",
      "epoch 208, loss 0.003747, train acc 0.999580, time 60.54 sec, lr 0.0010000000000000002\n",
      "epoch 209, loss 0.003782, train acc 0.999480, time 60.84 sec, lr 0.0010000000000000002\n",
      "epoch 210, loss 0.003786, train acc 0.999540, time 60.22 sec, lr 0.0010000000000000002\n",
      "epoch 211, loss 0.003854, train acc 0.999460, time 61.16 sec, lr 0.0010000000000000002\n",
      "epoch 212, loss 0.003864, train acc 0.999480, time 60.45 sec, lr 0.0010000000000000002\n",
      "epoch 213, loss 0.004602, train acc 0.999220, time 60.42 sec, lr 0.0010000000000000002\n",
      "epoch 214, loss 0.003850, train acc 0.999500, time 60.15 sec, lr 0.0010000000000000002\n",
      "epoch 215, loss 0.003551, train acc 0.999640, time 60.78 sec, lr 0.0010000000000000002\n",
      "epoch 216, loss 0.003775, train acc 0.999460, time 60.23 sec, lr 0.0010000000000000002\n",
      "epoch 217, loss 0.004038, train acc 0.999220, time 60.67 sec, lr 0.0010000000000000002\n",
      "epoch 218, loss 0.003870, train acc 0.999400, time 59.81 sec, lr 0.0010000000000000002\n",
      "epoch 219, loss 0.003750, train acc 0.999440, time 60.68 sec, lr 0.0010000000000000002\n",
      "epoch 220, loss 0.003495, train acc 0.999560, time 60.61 sec, lr 0.0010000000000000002\n",
      "epoch 221, loss 0.004183, train acc 0.999420, time 60.47 sec, lr 0.0010000000000000002\n",
      "epoch 222, loss 0.003975, train acc 0.999400, time 60.62 sec, lr 0.0010000000000000002\n",
      "epoch 223, loss 0.003971, train acc 0.999440, time 60.14 sec, lr 0.0010000000000000002\n",
      "epoch 224, loss 0.003740, train acc 0.999500, time 60.27 sec, lr 0.0010000000000000002\n",
      "epoch 225, loss 0.003587, train acc 0.999500, time 59.50 sec, lr 0.0010000000000000002\n",
      "epoch 226, loss 0.003398, train acc 0.999720, time 60.33 sec, lr 0.0010000000000000002\n",
      "epoch 227, loss 0.003340, train acc 0.999620, time 59.84 sec, lr 0.0010000000000000002\n",
      "epoch 228, loss 0.004062, train acc 0.999400, time 60.71 sec, lr 0.0010000000000000002\n",
      "epoch 229, loss 0.003472, train acc 0.999560, time 59.79 sec, lr 0.0010000000000000002\n",
      "epoch 230, loss 0.003423, train acc 0.999740, time 60.50 sec, lr 0.0010000000000000002\n",
      "epoch 231, loss 0.003547, train acc 0.999420, time 60.73 sec, lr 0.0010000000000000002\n",
      "epoch 232, loss 0.003688, train acc 0.999500, time 60.34 sec, lr 0.0010000000000000002\n",
      "epoch 233, loss 0.003417, train acc 0.999600, time 60.58 sec, lr 0.0010000000000000002\n",
      "epoch 234, loss 0.003402, train acc 0.999620, time 60.33 sec, lr 0.0010000000000000002\n",
      "epoch 235, loss 0.003616, train acc 0.999520, time 60.84 sec, lr 0.0010000000000000002\n",
      "epoch 236, loss 0.003579, train acc 0.999520, time 60.48 sec, lr 0.0010000000000000002\n",
      "epoch 237, loss 0.003337, train acc 0.999600, time 60.13 sec, lr 0.0010000000000000002\n",
      "epoch 238, loss 0.003524, train acc 0.999540, time 60.99 sec, lr 0.0010000000000000002\n",
      "epoch 239, loss 0.003550, train acc 0.999540, time 60.51 sec, lr 0.0010000000000000002\n",
      "epoch 240, loss 0.003220, train acc 0.999660, time 61.52 sec, lr 0.0010000000000000002\n",
      "epoch 241, loss 0.003347, train acc 0.999600, time 60.60 sec, lr 0.00010000000000000003\n",
      "epoch 242, loss 0.003396, train acc 0.999640, time 60.07 sec, lr 0.00010000000000000003\n",
      "epoch 243, loss 0.003252, train acc 0.999680, time 60.69 sec, lr 0.00010000000000000003\n",
      "epoch 244, loss 0.003190, train acc 0.999700, time 60.58 sec, lr 0.00010000000000000003\n",
      "epoch 245, loss 0.003361, train acc 0.999520, time 60.10 sec, lr 0.00010000000000000003\n",
      "epoch 246, loss 0.003250, train acc 0.999680, time 60.81 sec, lr 0.00010000000000000003\n",
      "epoch 247, loss 0.003473, train acc 0.999520, time 60.60 sec, lr 0.00010000000000000003\n",
      "epoch 248, loss 0.003171, train acc 0.999680, time 60.99 sec, lr 0.00010000000000000003\n",
      "epoch 249, loss 0.003143, train acc 0.999720, time 60.43 sec, lr 0.00010000000000000003\n",
      "epoch 250, loss 0.003224, train acc 0.999600, time 60.35 sec, lr 0.00010000000000000003\n",
      "epoch 251, loss 0.003051, train acc 0.999720, time 60.22 sec, lr 0.00010000000000000003\n",
      "epoch 252, loss 0.003122, train acc 0.999640, time 60.91 sec, lr 0.00010000000000000003\n",
      "epoch 253, loss 0.003138, train acc 0.999700, time 60.43 sec, lr 0.00010000000000000003\n",
      "epoch 254, loss 0.003225, train acc 0.999660, time 59.98 sec, lr 0.00010000000000000003\n",
      "epoch 255, loss 0.003187, train acc 0.999660, time 60.59 sec, lr 0.00010000000000000003\n",
      "epoch 256, loss 0.003069, train acc 0.999680, time 60.52 sec, lr 0.00010000000000000003\n",
      "epoch 257, loss 0.003247, train acc 0.999720, time 60.81 sec, lr 0.00010000000000000003\n",
      "epoch 258, loss 0.003098, train acc 0.999580, time 60.51 sec, lr 0.00010000000000000003\n",
      "epoch 259, loss 0.003066, train acc 0.999660, time 60.37 sec, lr 0.00010000000000000003\n",
      "epoch 260, loss 0.002927, train acc 0.999740, time 60.93 sec, lr 0.00010000000000000003\n",
      "epoch 261, loss 0.003076, train acc 0.999660, time 60.13 sec, lr 0.00010000000000000003\n",
      "epoch 262, loss 0.003173, train acc 0.999660, time 60.89 sec, lr 0.00010000000000000003\n",
      "epoch 263, loss 0.002974, train acc 0.999700, time 60.68 sec, lr 0.00010000000000000003\n",
      "epoch 264, loss 0.003277, train acc 0.999660, time 60.52 sec, lr 0.00010000000000000003\n",
      "epoch 265, loss 0.003103, train acc 0.999820, time 60.61 sec, lr 0.00010000000000000003\n",
      "epoch 266, loss 0.003184, train acc 0.999780, time 60.23 sec, lr 0.00010000000000000003\n",
      "epoch 267, loss 0.002850, train acc 0.999780, time 60.62 sec, lr 0.00010000000000000003\n",
      "epoch 268, loss 0.003249, train acc 0.999520, time 60.77 sec, lr 0.00010000000000000003\n",
      "epoch 269, loss 0.003040, train acc 0.999780, time 60.44 sec, lr 0.00010000000000000003\n",
      "epoch 270, loss 0.003104, train acc 0.999720, time 60.85 sec, lr 0.00010000000000000003\n",
      "epoch 271, loss 0.003184, train acc 0.999680, time 60.56 sec, lr 0.00010000000000000003\n",
      "epoch 272, loss 0.003278, train acc 0.999560, time 60.88 sec, lr 0.00010000000000000003\n",
      "epoch 273, loss 0.003099, train acc 0.999620, time 60.22 sec, lr 0.00010000000000000003\n",
      "epoch 274, loss 0.003237, train acc 0.999560, time 60.79 sec, lr 0.00010000000000000003\n",
      "epoch 275, loss 0.003004, train acc 0.999720, time 61.33 sec, lr 0.00010000000000000003\n",
      "epoch 276, loss 0.003027, train acc 0.999640, time 60.62 sec, lr 0.00010000000000000003\n",
      "epoch 277, loss 0.003135, train acc 0.999620, time 60.74 sec, lr 0.00010000000000000003\n",
      "epoch 278, loss 0.003329, train acc 0.999700, time 60.60 sec, lr 0.00010000000000000003\n",
      "epoch 279, loss 0.003001, train acc 0.999720, time 60.70 sec, lr 0.00010000000000000003\n",
      "epoch 280, loss 0.003007, train acc 0.999780, time 60.20 sec, lr 0.00010000000000000003\n",
      "epoch 281, loss 0.003354, train acc 0.999500, time 60.09 sec, lr 0.00010000000000000003\n",
      "epoch 282, loss 0.003039, train acc 0.999740, time 60.51 sec, lr 0.00010000000000000003\n",
      "epoch 283, loss 0.002945, train acc 0.999700, time 60.13 sec, lr 0.00010000000000000003\n",
      "epoch 284, loss 0.003130, train acc 0.999680, time 60.75 sec, lr 0.00010000000000000003\n",
      "epoch 285, loss 0.003104, train acc 0.999640, time 61.05 sec, lr 0.00010000000000000003\n",
      "epoch 286, loss 0.003183, train acc 0.999540, time 61.01 sec, lr 0.00010000000000000003\n",
      "epoch 287, loss 0.003203, train acc 0.999520, time 61.47 sec, lr 0.00010000000000000003\n",
      "epoch 288, loss 0.003028, train acc 0.999720, time 61.06 sec, lr 0.00010000000000000003\n",
      "epoch 289, loss 0.003425, train acc 0.999440, time 60.64 sec, lr 0.00010000000000000003\n",
      "epoch 290, loss 0.002949, train acc 0.999760, time 60.22 sec, lr 0.00010000000000000003\n",
      "epoch 291, loss 0.002959, train acc 0.999740, time 60.30 sec, lr 0.00010000000000000003\n",
      "epoch 292, loss 0.002962, train acc 0.999720, time 60.98 sec, lr 0.00010000000000000003\n",
      "epoch 293, loss 0.003490, train acc 0.999360, time 60.72 sec, lr 0.00010000000000000003\n",
      "epoch 294, loss 0.003169, train acc 0.999600, time 60.85 sec, lr 0.00010000000000000003\n",
      "epoch 295, loss 0.003168, train acc 0.999620, time 61.45 sec, lr 0.00010000000000000003\n",
      "epoch 296, loss 0.002955, train acc 0.999720, time 60.81 sec, lr 0.00010000000000000003\n",
      "epoch 297, loss 0.003132, train acc 0.999720, time 61.09 sec, lr 0.00010000000000000003\n",
      "epoch 298, loss 0.002647, train acc 0.999820, time 60.00 sec, lr 0.00010000000000000003\n",
      "epoch 299, loss 0.003100, train acc 0.999620, time 59.97 sec, lr 0.00010000000000000003\n",
      "epoch 300, loss 0.003011, train acc 0.999740, time 61.11 sec, lr 0.00010000000000000003\n",
      "Current used time : 18164.43522787094\n"
     ]
    }
   ],
   "source": [
    "net, preds = get_net(ctx), []\n",
    "net.hybridize()\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)\n",
    "time_second = time.time()\n",
    "print(\"Current used time : {}\".format(time_second - time_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.io.formats.csvs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-75cd1bf6e9a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msorted_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_valid_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.io.formats.csvs'"
     ]
    }
   ],
   "source": [
    "for X, _ in test_iter:\n",
    "    y_hat = net(X.as_in_ctx(ctx))\n",
    "    preds.extend(y_hat.argmax(axis=1).astype(int).asnumpy())\n",
    "sorted_ids = list(range(1, len(test_ds) + 1))\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "After executing the above code, we will get a \"submission.csv\" file. The format\n",
    "of this file is consistent with the Kaggle competition requirements. The method\n",
    "for submitting results is similar to method in :numref:`sec_kaggle_house`.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* We can create an `ImageFolderDataset` instance to read the dataset containing the original image files.\n",
    "* We can use convolutional neural networks, image augmentation, and hybrid programming to take part in an image classification competition.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Use the complete CIFAF-10 dataset for the Kaggle competition. Change the `batch_size` and number of epochs `num_epochs` to 128 and 100, respectively.  See what accuracy and ranking you can achieve in this competition.\n",
    "1. What accuracy can you achieve when not using image augmentation?\n",
    "1. Scan the QR code to access the relevant discussions and exchange ideas about the methods used and the results obtained with the community. Can you come up with any better techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/379)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
