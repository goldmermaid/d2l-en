{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Dog Breed Identification (ImageNet Dogs) on Kaggle\n",
    "\n",
    "\n",
    "In this section, we will tackle the dog breed identification challenge in the Kaggle Competition. The competition's web address is\n",
    "\n",
    "> https://www.kaggle.com/c/dog-breed-identification\n",
    "\n",
    "In this competition, we attempt to identify 120 different breeds of dogs. The dataset used in this competition is actually a subset of the famous ImageNet dataset. Different from the images in the CIFAR-10 dataset used in the previous section, the images in the ImageNet dataset are higher and wider and their dimensions are inconsistent.\n",
    "\n",
    ":numref:`fig_kaggle_dog` shows the information on the competition's webpage. In order to submit the results, please register an account on the Kaggle website first.\n",
    "\n",
    "![Dog breed identification competition website. The dataset for the competition can be accessed by clicking the \"Data\" tab.](../img/kaggle-dog.jpg)\n",
    ":width:`400px`\n",
    ":label:`fig_kaggle_dog`\n",
    "\n",
    "First, import the packages or modules required for the competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from d2l import mxnet as d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, npx\n",
    "from mxnet.gluon import nn\n",
    "import os\n",
    "import time\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "## Obtaining and Organizing the Dataset\n",
    "\n",
    "The competition data is divided into a training set and testing set. The training set contains $10,222$ images and the testing set contains $10,357$ images. The images in both sets are in JPEG format. These images contain three RGB channels (color) and they have different heights and widths. There are 120 breeds of dogs in the training set, including Labradors, Poodles, Dachshunds, Samoyeds, Huskies, Chihuahuas, and Yorkshire Terriers.\n",
    "\n",
    "### Downloading the Dataset\n",
    "\n",
    "After logging in to Kaggle, we can click on the \"Data\" tab on the dog breed identification competition webpage shown in :numref:`fig_kaggle_dog` and download the dataset by clicking the \"Download All\" button. After unzipping the downloaded file in `../data`, you will find the entire dataset in the following paths:\n",
    "\n",
    "* ../data/dog-breed-identification/labels.csv\n",
    "* ../data/dog-breed-identification/sample_submission.csv\n",
    "* ../data/dog-breed-identification/train\n",
    "* ../data/dog-breed-identification/test\n",
    "\n",
    "You may have noticed that the above structure is quite similar to that of the CIFAR-10 competition in :numref:`sec_kaggle_cifar10`, where folders `train/` and `test/` contain training and testing dog images respectively, and `labels.csv` has the labels for the training images.\n",
    "\n",
    "Similarly, to make it easier to get started, we provide a small-scale sample of the dataset mentioned above, \"train_valid_test_tiny.zip\". If you are going to use the full dataset for the Kaggle competition, you will also need to change the `demo` variable below to `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "origin_pos": 3,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',\n",
    "                            '7c9b54e78c1cedaa04998f9868bc548c60101362')\n",
    "\n",
    "# If you use the full dataset downloaded for the Kaggle competition, change\n",
    "# the variable below to False\n",
    "demo = False\n",
    "if demo:\n",
    "    data_dir = d2l.download_extract('dog_tiny')\n",
    "else:\n",
    "    data_dir = os.path.join('..', 'data', 'dog-breed-identification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/dog-breed-identification'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "### Organizing the Dataset\n",
    "\n",
    "We can organize the dataset similarly to what we did in :numref:`sec_kaggle_cifar10`, namely separating a validation set from the training set, and moving images into subfolders grouped by labels.\n",
    "\n",
    "The `reorg_dog_data` function below is used to read the training data labels, segment the validation set, and organize the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "origin_pos": 5,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def reorg_dog_data(data_dir, valid_ratio):\n",
    "    labels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv'))\n",
    "    d2l.reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    d2l.reorg_test(data_dir)\n",
    "\n",
    "batch_size = 1 if demo else 128\n",
    "valid_ratio = 0.1\n",
    "reorg_dog_data(data_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## Image Augmentation\n",
    "\n",
    "The size of the images in this section are larger than the images in the previous section. Here are some more image augmentation operations that might be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "origin_pos": 7,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "transform_train = gluon.data.vision.transforms.Compose([\n",
    "    # Randomly crop the image to obtain an image with an area of 0.08 to 1 of\n",
    "    # the original area and height to width ratio between 3/4 and 4/3. Then,\n",
    "    # scale the image to create a new image with a height and width of 224\n",
    "    # pixels each\n",
    "    gluon.data.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
    "                                                   ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    gluon.data.vision.transforms.RandomFlipLeftRight(),\n",
    "    # Randomly change the brightness, contrast, and saturation\n",
    "    gluon.data.vision.transforms.RandomColorJitter(brightness=0.4,\n",
    "                                                   contrast=0.4,\n",
    "                                                   saturation=0.4),\n",
    "    # Add random noise\n",
    "    gluon.data.vision.transforms.RandomLighting(0.1),\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    # Standardize each channel of the image\n",
    "    gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "During testing, we only use definite image preprocessing operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "transform_test = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.Resize(256),\n",
    "    # Crop a square of 224 by 224 from the center of the image\n",
    "    gluon.data.vision.transforms.CenterCrop(224),\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "As in the previous section, we can create an `ImageFolderDataset` instance to read the dataset containing the original image files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "origin_pos": 11,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "train_ds, valid_ds, train_valid_ds, test_ds = [\n",
    "    gluon.data.vision.ImageFolderDataset(\n",
    "        os.path.join(data_dir, 'train_valid_test', folder))\n",
    "    for folder in ('train', 'valid', 'train_valid', 'test')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Here, we create `DataLoader` instances, just like in :numref:`sec_kaggle_cifar10`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [gluon.data.DataLoader(\n",
    "    dataset.transform_first(transform_train), batch_size, shuffle=True,\n",
    "    last_batch='keep') for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter, test_iter = [gluon.data.DataLoader(\n",
    "    dataset.transform_first(transform_test), batch_size, shuffle=False,\n",
    "    last_batch='keep') for dataset in (valid_ds, test_ds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "The dataset for this competition is a subset of the ImageNet data\n",
    "set. Therefore, we can use the approach discussed in\n",
    ":numref:`sec_fine_tuning`\n",
    "to select a model pre-trained on the\n",
    "entire ImageNet dataset and use it to extract image features to be input in the\n",
    "custom small-scale output network. Gluon provides a wide range of pre-trained\n",
    "models. Here, we will use the pre-trained ResNet-34 model. Because the\n",
    "competition dataset is a subset of the pre-training dataset, we simply reuse\n",
    "the input of the pre-trained model's output layer, i.e., the extracted\n",
    "features. Then, we can replace the original output layer with a small custom\n",
    "output network that can be trained, such as two fully connected layers in a\n",
    "series. Different from the experiment in\n",
    ":numref:`sec_fine_tuning`, here, we do not retrain the pre-trained model used for feature\n",
    "extraction. This reduces the training time and the memory required to store\n",
    "model parameter gradients.\n",
    "\n",
    "You must note that, during image augmentation, we use the mean values and standard deviations of the three RGB channels for the entire ImageNet dataset for normalization. This is consistent with the normalization of the pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "origin_pos": 15,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def get_net(ctx):\n",
    "    finetune_net = gluon.model_zoo.vision.resnet34_v2(pretrained=True)\n",
    "    # Define a new output network\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    # There are 120 output categories\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # Initialize the output network\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
    "    # Distribute the model parameters to the CPUs or GPUs used for computation\n",
    "    finetune_net.collect_params().reset_ctx(ctx)\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "When calculating the loss, we first use the member variable `features` to obtain the input of the pre-trained model's output layer, i.e., the extracted feature. Then, we use this feature as the input for our small custom output network and compute the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def evaluate_loss(data_iter, net, ctx):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.as_in_ctx(ctx)\n",
    "        output_features = net.features(X.as_in_ctx(ctx))\n",
    "        outputs = net.output_new(output_features)\n",
    "        l_sum += float(loss(outputs, y).sum())\n",
    "        n += y.size\n",
    "    return l_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## Defining the Training Functions\n",
    "\n",
    "We will select the model and tune hyperparameters according to the model's performance on the validation set. The model training function `train` only trains the small custom output network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "origin_pos": 19,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    # Only train the small custom output network\n",
    "    trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, n, start = 0.0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in train_iter:\n",
    "            y = y.as_in_ctx(ctx)\n",
    "            output_features = net.features(X.as_in_ctx(ctx))\n",
    "            with autograd.record():\n",
    "                outputs = net.output_new(output_features)\n",
    "                l = loss(outputs, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += float(l)\n",
    "            n += y.size\n",
    "        time_s = f'time {time.time() - start:.2f} sec'\n",
    "        if valid_iter is not None:\n",
    "            valid_loss = evaluate_loss(valid_iter, net, ctx)\n",
    "            epoch_s = (f'epoch {epoch + 1}, train loss {train_l_sum / n:f}, '\n",
    "                       f'valid loss {valid_loss:f}, ')\n",
    "        else:\n",
    "            epoch_s = f'epoch {epoch + 1}, train loss {train_l_sum / n:f}, '\n",
    "        print(epoch_s + time_s + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "## Training and Validating the Model\n",
    "\n",
    "Now, we can train and validate the model. The following hyperparameters can be tuned. For example, we can increase the number of epochs. Because `lr_period` and `lr_decay` are set to 10 and 0.1 respectively, the learning rate of the optimization algorithm will be multiplied by 0.1 after every 10 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "origin_pos": 21,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ubuntu/.mxnet/models/resnet34_v2-9d6b80bb.zip5d0dd228-39f4-4fd9-9913-80d381651f2c from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet34_v2-9d6b80bb.zip...\n",
      "epoch 1, train loss 3.347879, valid loss 1.295898, time 53.25 sec, lr 0.01\n",
      "epoch 2, train loss 1.314158, valid loss 0.715496, time 51.48 sec, lr 0.01\n",
      "epoch 3, train loss 1.030652, valid loss 0.575786, time 51.81 sec, lr 0.01\n",
      "epoch 4, train loss 0.926444, valid loss 0.548434, time 51.85 sec, lr 0.01\n",
      "epoch 5, train loss 0.908059, valid loss 0.514701, time 51.69 sec, lr 0.01\n",
      "epoch 6, train loss 0.856510, valid loss 0.507508, time 51.86 sec, lr 0.01\n",
      "epoch 7, train loss 0.824687, valid loss 0.500920, time 51.91 sec, lr 0.01\n",
      "epoch 8, train loss 0.835241, valid loss 0.483979, time 51.83 sec, lr 0.01\n",
      "epoch 9, train loss 0.791878, valid loss 0.467520, time 51.87 sec, lr 0.01\n",
      "epoch 10, train loss 0.796793, valid loss 0.462794, time 51.98 sec, lr 0.01\n",
      "epoch 11, train loss 0.775709, valid loss 0.431332, time 51.84 sec, lr 0.001\n",
      "epoch 12, train loss 0.708309, valid loss 0.423410, time 51.93 sec, lr 0.001\n",
      "epoch 13, train loss 0.701369, valid loss 0.418658, time 51.79 sec, lr 0.001\n",
      "epoch 14, train loss 0.696897, valid loss 0.418874, time 51.94 sec, lr 0.001\n",
      "epoch 15, train loss 0.690909, valid loss 0.419208, time 52.07 sec, lr 0.001\n",
      "epoch 16, train loss 0.712716, valid loss 0.418254, time 51.95 sec, lr 0.001\n",
      "epoch 17, train loss 0.720665, valid loss 0.416367, time 51.65 sec, lr 0.001\n",
      "epoch 18, train loss 0.696836, valid loss 0.415937, time 51.87 sec, lr 0.001\n",
      "epoch 19, train loss 0.697419, valid loss 0.410975, time 51.84 sec, lr 0.001\n",
      "epoch 20, train loss 0.705275, valid loss 0.412986, time 51.80 sec, lr 0.001\n",
      "epoch 21, train loss 0.699988, valid loss 0.412759, time 51.73 sec, lr 0.0001\n",
      "epoch 22, train loss 0.685143, valid loss 0.412039, time 51.94 sec, lr 0.0001\n",
      "epoch 23, train loss 0.701158, valid loss 0.411864, time 51.77 sec, lr 0.0001\n",
      "epoch 24, train loss 0.699151, valid loss 0.411998, time 51.87 sec, lr 0.0001\n",
      "epoch 25, train loss 0.709966, valid loss 0.412325, time 51.71 sec, lr 0.0001\n",
      "epoch 26, train loss 0.713049, valid loss 0.412733, time 51.81 sec, lr 0.0001\n",
      "epoch 27, train loss 0.692698, valid loss 0.412512, time 51.96 sec, lr 0.0001\n",
      "epoch 28, train loss 0.693069, valid loss 0.412139, time 51.91 sec, lr 0.0001\n",
      "epoch 29, train loss 0.699231, valid loss 0.412595, time 51.97 sec, lr 0.0001\n",
      "epoch 30, train loss 0.683182, valid loss 0.412451, time 51.91 sec, lr 0.0001\n",
      "epoch 31, train loss 0.695315, valid loss 0.412274, time 51.90 sec, lr 1e-05\n",
      "epoch 32, train loss 0.675839, valid loss 0.412248, time 52.01 sec, lr 1e-05\n",
      "epoch 33, train loss 0.674927, valid loss 0.412214, time 51.90 sec, lr 1e-05\n",
      "epoch 34, train loss 0.692163, valid loss 0.412178, time 51.92 sec, lr 1e-05\n",
      "epoch 35, train loss 0.684498, valid loss 0.412174, time 51.81 sec, lr 1e-05\n",
      "epoch 36, train loss 0.692535, valid loss 0.412181, time 51.91 sec, lr 1e-05\n",
      "epoch 37, train loss 0.684827, valid loss 0.412190, time 51.77 sec, lr 1e-05\n",
      "epoch 38, train loss 0.697203, valid loss 0.412180, time 51.64 sec, lr 1e-05\n",
      "epoch 39, train loss 0.686229, valid loss 0.412227, time 51.92 sec, lr 1e-05\n",
      "epoch 40, train loss 0.687285, valid loss 0.412210, time 51.90 sec, lr 1e-05\n",
      "epoch 41, train loss 0.687381, valid loss 0.412209, time 51.89 sec, lr 1.0000000000000002e-06\n",
      "epoch 42, train loss 0.693841, valid loss 0.412209, time 51.91 sec, lr 1.0000000000000002e-06\n",
      "epoch 43, train loss 0.696889, valid loss 0.412202, time 51.90 sec, lr 1.0000000000000002e-06\n",
      "epoch 44, train loss 0.694925, valid loss 0.412196, time 51.75 sec, lr 1.0000000000000002e-06\n",
      "epoch 45, train loss 0.711976, valid loss 0.412195, time 51.95 sec, lr 1.0000000000000002e-06\n",
      "epoch 46, train loss 0.703931, valid loss 0.412197, time 51.77 sec, lr 1.0000000000000002e-06\n",
      "epoch 47, train loss 0.700283, valid loss 0.412201, time 51.97 sec, lr 1.0000000000000002e-06\n",
      "epoch 48, train loss 0.701102, valid loss 0.412197, time 51.65 sec, lr 1.0000000000000002e-06\n",
      "epoch 49, train loss 0.677886, valid loss 0.412193, time 51.88 sec, lr 1.0000000000000002e-06\n",
      "epoch 50, train loss 0.679411, valid loss 0.412186, time 51.92 sec, lr 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 50, 0.01, 1e-4\n",
    "lr_period, lr_decay, net = 10, 0.1, get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "## Classifying the Testing Set and Submitting Results on Kaggle\n",
    "\n",
    "After obtaining a satisfactory model design and hyperparameters, we use all training datasets (including validation sets) to retrain the model and then classify the testing set. Note that predictions are made by the output network we just trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "origin_pos": 23,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 3.229150, time 56.42 sec, lr 0.01\n",
      "epoch 2, train loss 1.240590, time 55.70 sec, lr 0.01\n",
      "epoch 3, train loss 0.975248, time 55.85 sec, lr 0.01\n",
      "epoch 4, train loss 0.924296, time 55.81 sec, lr 0.01\n",
      "epoch 5, train loss 0.870770, time 55.83 sec, lr 0.01\n",
      "epoch 6, train loss 0.848682, time 55.83 sec, lr 0.01\n",
      "epoch 7, train loss 0.846281, time 55.88 sec, lr 0.01\n",
      "epoch 8, train loss 0.826951, time 55.83 sec, lr 0.01\n",
      "epoch 9, train loss 0.805177, time 56.03 sec, lr 0.01\n",
      "epoch 10, train loss 0.781458, time 55.84 sec, lr 0.01\n",
      "epoch 11, train loss 0.753358, time 55.72 sec, lr 0.001\n",
      "epoch 12, train loss 0.704732, time 55.89 sec, lr 0.001\n",
      "epoch 13, train loss 0.710450, time 55.88 sec, lr 0.001\n",
      "epoch 14, train loss 0.697369, time 55.84 sec, lr 0.001\n",
      "epoch 15, train loss 0.703750, time 55.83 sec, lr 0.001\n",
      "epoch 16, train loss 0.710672, time 55.87 sec, lr 0.001\n",
      "epoch 17, train loss 0.698553, time 55.83 sec, lr 0.001\n",
      "epoch 18, train loss 0.705530, time 55.75 sec, lr 0.001\n",
      "epoch 19, train loss 0.710219, time 55.97 sec, lr 0.001\n",
      "epoch 20, train loss 0.693516, time 55.87 sec, lr 0.001\n",
      "epoch 21, train loss 0.704960, time 55.98 sec, lr 0.0001\n",
      "epoch 22, train loss 0.702746, time 55.96 sec, lr 0.0001\n",
      "epoch 23, train loss 0.695337, time 56.13 sec, lr 0.0001\n",
      "epoch 24, train loss 0.686747, time 55.86 sec, lr 0.0001\n",
      "epoch 25, train loss 0.676338, time 55.98 sec, lr 0.0001\n",
      "epoch 26, train loss 0.700627, time 55.87 sec, lr 0.0001\n",
      "epoch 27, train loss 0.713502, time 55.95 sec, lr 0.0001\n",
      "epoch 28, train loss 0.684844, time 56.01 sec, lr 0.0001\n",
      "epoch 29, train loss 0.702319, time 56.12 sec, lr 0.0001\n",
      "epoch 30, train loss 0.677849, time 55.92 sec, lr 0.0001\n",
      "epoch 31, train loss 0.676552, time 55.98 sec, lr 1e-05\n",
      "epoch 32, train loss 0.702840, time 55.81 sec, lr 1e-05\n",
      "epoch 33, train loss 0.676395, time 55.97 sec, lr 1e-05\n",
      "epoch 34, train loss 0.708704, time 55.81 sec, lr 1e-05\n",
      "epoch 35, train loss 0.693783, time 55.83 sec, lr 1e-05\n",
      "epoch 36, train loss 0.699714, time 56.01 sec, lr 1e-05\n",
      "epoch 37, train loss 0.674585, time 55.88 sec, lr 1e-05\n",
      "epoch 38, train loss 0.705031, time 55.93 sec, lr 1e-05\n",
      "epoch 39, train loss 0.685215, time 55.78 sec, lr 1e-05\n",
      "epoch 40, train loss 0.689802, time 55.76 sec, lr 1e-05\n",
      "epoch 41, train loss 0.709668, time 55.96 sec, lr 1.0000000000000002e-06\n",
      "epoch 42, train loss 0.694273, time 55.92 sec, lr 1.0000000000000002e-06\n",
      "epoch 43, train loss 0.704007, time 55.82 sec, lr 1.0000000000000002e-06\n",
      "epoch 44, train loss 0.676156, time 55.95 sec, lr 1.0000000000000002e-06\n",
      "epoch 45, train loss 0.684226, time 55.75 sec, lr 1.0000000000000002e-06\n",
      "epoch 46, train loss 0.693911, time 55.71 sec, lr 1.0000000000000002e-06\n",
      "epoch 47, train loss 0.676737, time 55.95 sec, lr 1.0000000000000002e-06\n",
      "epoch 48, train loss 0.681184, time 55.93 sec, lr 1.0000000000000002e-06\n",
      "epoch 49, train loss 0.712461, time 55.76 sec, lr 1.0000000000000002e-06\n",
      "epoch 50, train loss 0.686916, time 55.83 sec, lr 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "preds = []\n",
    "for data, label in test_iter:\n",
    "    output_features = net.features(data.as_in_ctx(ctx))\n",
    "    output = npx.softmax(net.output_new(output_features))\n",
    "    preds.extend(output.asnumpy())\n",
    "ids = sorted(os.listdir(\n",
    "    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "After executing the above code, we will generate a \"submission.csv\" file. The\n",
    "format of this file is consistent with the Kaggle competition requirements. The\n",
    "method for submitting results is similar to method in\n",
    ":numref:`sec_kaggle_house`.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* We can use a model pre-trained on the ImageNet dataset to extract features and only train a small custom output network. This will allow us to classify a subset of the ImageNet dataset with lower computing and storage overhead.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. When using the entire Kaggle dataset, what kind of results do you get when you increase the `batch_size` (batch size) and `num_epochs` (number of epochs)?\n",
    "1. Do you get better results if you use a deeper pre-trained model?\n",
    "1. Scan the QR code to access the relevant discussions and exchange ideas about the methods used and the results obtained with the community. Can you come up with any better techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/380)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
